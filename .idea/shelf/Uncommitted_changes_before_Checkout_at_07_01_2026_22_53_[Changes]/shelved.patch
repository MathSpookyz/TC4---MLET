Index: README.md
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># API de Previs√£o de Pre√ßos de A√ß√µes\n\nSistema completo de previs√£o de pre√ßos de a√ß√µes usando modelo LSTM (Long Short-Term Memory) com pipeline ETL automatizado, API REST e **monitoramento MLFlow sempre habilitado**.\n\n## \uD83D\uDE80 Caracter√≠sticas Principais\n\n- ‚úÖ **Multi-ticker**: Suporta qualquer a√ß√£o da bolsa brasileira\n- ‚úÖ **Treinamento autom√°tico**: Treina modelos sob demanda quando necess√°rio\n- ‚úÖ **Armazenamento h√≠brido**: Local ou S3 via vari√°vel de ambiente\n- ‚úÖ **MLFlow sempre ativo**: Rastreamento autom√°tico de todas as opera√ß√µes\n- ‚úÖ **Dados personalizados**: Endpoint para treinar/prever com seus pr√≥prios dados\n- ‚úÖ **Logs detalhados**: Rastreamento completo de opera√ß√µes\n- ‚úÖ **Cache inteligente**: Modelos permanecem em mem√≥ria ap√≥s carregamento\n\n## \uD83D\uDCCB Requisitos\n\n- Python 3.11+\n- Docker (opcional)\n\n## ‚ö° In√≠cio R√°pido\n\n### 1. Instalar Depend√™ncias\n\n```bash\npip install -r requirements.txt\n```\n\n### 2. Configurar Ambiente (Opcional)\n\nCrie `.env` na raiz do projeto:\n\n```bash\n# Armazenamento (padr√£o: local)\nSTORAGE_TYPE=local\n\n# Para usar S3:\n# STORAGE_TYPE=s3\n# S3_BUCKET=seu-bucket\n# AWS_ACCESS_KEY_ID=sua-chave\n# AWS_SECRET_ACCESS_KEY=seu-secret\n\n# MLFlow (padr√£o: local)\nMLFLOW_TRACKING_URI=file:./mlruns\n```\n\n### 3. Iniciar API\n\n```bash\npython api.py\n```\n\nAcesse: http://localhost:8000/docs\n\n### 4. Visualizar Experimentos MLFlow (Opcional)\n\n```bash\nmlflow ui\n```\n\nAcesse: http://localhost:5000\n\nO MLFlow rastreia automaticamente:\n- Par√¢metros de treinamento\n- M√©tricas (RMSE, previs√µes)\n- Modelos e artefatos\n- Hist√≥rico completo de opera√ß√µes\n\n\uD83D\uDCD6 **Guia completo**: [MLFLOW_GUIDE.md](MLFLOW_GUIDE.md)\n\n## \uD83D\uDCCA Como Usar\n\n### Treinar um Modelo\n\n#### Via Linha de Comando\n\n```bash\npython model/model_training.py\n```\n\nO sistema solicitar√° o ticker (ex: PETR4.SA, VALE3.SA, ITUB4.SA)\n\n#### Via API\n\n```bash\n# Treinar modelo via API\ncurl -X POST http://localhost:8000/train \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"ticker\": \"VALE3.SA\", \"start_date\": \"2020-01-01\"}'\n```\n\n### Fazer Previs√µes\n\n#### Previs√£o Simples (com treinamento autom√°tico)\n\n```bash\n# O sistema treina automaticamente se o modelo n√£o existir\ncurl \"http://localhost:8000/predict/PETR4.SA?days=7\"\n```\n\n#### Via API - GET\n\n```bash\n# Previs√£o de 1 dia para VALE3.SA\ncurl http://localhost:8000/predict/VALE3.SA\n\n# Previs√£o de 7 dias\ncurl \"http://localhost:8000/predict/VALE3.SA?days=7\"\n\n# Com per√≠odo espec√≠fico\ncurl \"http://localhost:8000/predict/ITUB4.SA?days=5&start_date=2023-01-01&end_date=2024-12-31\"\n```\n\n#### Via API - POST\n\n```bash\ncurl -X POST http://localhost:8000/predict \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"ticker\": \"PETR4.SA\",\n    \"days\": 30,\n    \"start_date\": \"2023-01-01\",\n    \"end_date\": \"2024-12-31\"\n  }'\n```\n\n### Estrutura de Resposta\n\n#### Treinamento\n\n```json\n{\n  \"ticker\": \"VALE3.SA\",\n  \"status\": \"success\",\n  \"message\": \"Modelo treinado com sucesso para VALE3.SA\",\n  \"rmse\": 2.45,\n  \"next_prediction\": 65.32,\n  \"trained_at\": \"2026-01-07T10:30:00.000000\"\n}\n```\n\n#### Previs√£o\n\n```json\n{\n  \"ticker\": \"VALE3.SA\",\n  \"predictions\": [65.32, 65.87, 66.15],\n  \"days\": 3,\n  \"last_known_price\": 64.80,\n  \"currency\": \"BRL\"\n}\n```\n\n### Usar Dados Personalizados\n\nO endpoint `/predict-custom` permite treinar um modelo tempor√°rio com seus pr√≥prios dados hist√≥ricos. Ideal para:\n\n- Testar estrat√©gias com dados hist√≥ricos espec√≠ficos\n- Fazer previs√µes com dados sint√©ticos ou simulados\n- Validar o modelo sem interferir com modelos salvos\n- Treinar e prever sem depend√™ncia de tickers da bolsa\n\n#### Caracter√≠sticas do Endpoint Custom\n\n‚úÖ **Isolado**: N√£o salva o modelo no disco  \n‚úÖ **Tempor√°rio**: Modelo existe apenas durante a requisi√ß√£o  \n‚úÖ **Flex√≠vel**: Aceita qualquer conjunto de dados hist√≥ricos  \n‚úÖ **Completo**: Retorna m√©tricas de treinamento (RMSE)\n\n#### Requisitos de Dados\n\n- **M√≠nimo**: 30 pontos hist√≥ricos\n- **Recomendado**: 60+ pontos para melhor acur√°cia\n- **Formato**: JSON com date, close e volume\n\n#### Exemplo de Uso\n\n```bash\ncurl -X POST http://localhost:8000/predict-custom \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"ticker_name\": \"TESTE\",\n    \"days\": 5,\n    \"historical_data\": [\n      {\"date\": \"2024-01-01\", \"close\": 100.5, \"volume\": 1000000},\n      {\"date\": \"2024-01-02\", \"close\": 101.2, \"volume\": 1100000},\n      ... (m√≠nimo 30 pontos)\n    ]\n  }'\n```\n\n#### Usando Arquivo JSON\n\n```bash\n# Usar dados do arquivo de exemplo inclu√≠do no projeto\ncurl -X POST http://localhost:8000/predict-custom \\\n  -H \"Content-Type: application/json\" \\\n  -d @example_custom_data.json\n```\n\n#### Teste R√°pido\n\n```bash\ncurl -X POST http://localhost:8000/predict-custom \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"ticker_name\": \"TESTE_RAPIDO\",\n    \"days\": 3,\n    \"historical_data\": [\n      {\"date\": \"2024-01-01\", \"close\": 100.00, \"volume\": 1000000},\n      {\"date\": \"2024-01-02\", \"close\": 101.50, \"volume\": 1050000},\n      {\"date\": \"2024-01-03\", \"close\": 102.30, \"volume\": 1100000},\n      {\"date\": \"2024-01-04\", \"close\": 101.80, \"volume\": 1080000},\n      {\"date\": \"2024-01-05\", \"close\": 103.20, \"volume\": 1120000},\n      {\"date\": \"2024-01-08\", \"close\": 104.50, \"volume\": 1150000},\n      {\"date\": \"2024-01-09\", \"close\": 104.00, \"volume\": 1130000},\n      {\"date\": \"2024-01-10\", \"close\": 105.30, \"volume\": 1180000},\n      {\"date\": \"2024-01-11\", \"close\": 106.20, \"volume\": 1200000},\n      {\"date\": \"2024-01-12\", \"close\": 105.80, \"volume\": 1190000},\n      {\"date\": \"2024-01-15\", \"close\": 107.50, \"volume\": 1220000},\n      {\"date\": \"2024-01-16\", \"close\": 108.10, \"volume\": 1250000},\n      {\"date\": \"2024-01-17\", \"close\": 108.90, \"volume\": 1260000},\n      {\"date\": \"2024-01-18\", \"close\": 108.30, \"volume\": 1240000},\n      {\"date\": \"2024-01-19\", \"close\": 109.80, \"volume\": 1280000},\n      {\"date\": \"2024-01-22\", \"close\": 110.50, \"volume\": 1300000},\n      {\"date\": \"2024-01-23\", \"close\": 110.20, \"volume\": 1290000},\n      {\"date\": \"2024-01-24\", \"close\": 111.60, \"volume\": 1320000},\n      {\"date\": \"2024-01-25\", \"close\": 112.30, \"volume\": 1350000},\n      {\"date\": \"2024-01-26\", \"close\": 111.90, \"volume\": 1340000},\n      {\"date\": \"2024-01-29\", \"close\": 113.20, \"volume\": 1370000},\n      {\"date\": \"2024-01-30\", \"close\": 114.00, \"volume\": 1400000},\n      {\"date\": \"2024-01-31\", \"close\": 113.60, \"volume\": 1390000},\n      {\"date\": \"2024-02-01\", \"close\": 114.80, \"volume\": 1420000},\n      {\"date\": \"2024-02-02\", \"close\": 115.50, \"volume\": 1450000},\n      {\"date\": \"2024-02-05\", \"close\": 115.20, \"volume\": 1440000},\n      {\"date\": \"2024-02-06\", \"close\": 116.30, \"volume\": 1470000},\n      {\"date\": \"2024-02-07\", \"close\": 117.00, \"volume\": 1490000},\n      {\"date\": \"2024-02-08\", \"close\": 116.70, \"volume\": 1480000},\n      {\"date\": \"2024-02-09\", \"close\": 117.90, \"volume\": 1510000}\n    ]\n  }'\n```\n\n**Resposta esperada:**\n```json\n{\n  \"ticker_name\": \"TESTE_RAPIDO\",\n  \"predictions\": [118.45, 119.12, 119.78],\n  \"days\": 3,\n  \"last_known_price\": 117.90,\n  \"rmse\": 0.87,\n  \"training_samples\": 0,\n  \"message\": \"Modelo treinado e previs√£o realizada com sucesso usando 30 pontos hist√≥ricos\"\n}\n```\n\n#### Estrutura dos Dados\n\nCada ponto hist√≥rico deve conter:\n\n```json\n{\n  \"date\": \"YYYY-MM-DD\",\n  \"close\": float,\n  \"volume\": int\n}\n```\n\n#### Par√¢metros\n\n| Campo | Tipo | Obrigat√≥rio | Descri√ß√£o |\n|-------|------|-------------|-----------|\n| `ticker_name` | string | N√£o | Nome para identifica√ß√£o (padr√£o: \"CUSTOM\") |\n| `days` | integer | N√£o | Dias para prever (padr√£o: 1) |\n| `historical_data` | array | Sim | Lista de pontos hist√≥ricos (m√≠nimo 30) |\n\n#### Resposta do Endpoint Custom\n\n```json\n{\n  \"ticker_name\": \"MINHA_ACAO\",\n  \"predictions\": [102.45, 103.12, 103.78, 104.21, 104.67],\n  \"days\": 5,\n  \"last_known_price\": 101.80,\n  \"rmse\": 1.23,\n  \"training_samples\": 120,\n  \"message\": \"Modelo treinado e previs√£o realizada com sucesso usando 150 pontos hist√≥ricos\"\n}\n```\n\n#### Campos da Resposta\n\n| Campo | Tipo | Descri√ß√£o |\n|-------|------|-----------|\n| `ticker_name` | string | Nome fornecido na requisi√ß√£o |\n| `predictions` | array | Lista de previs√µes (um valor por dia) |\n| `days` | integer | N√∫mero de dias previstos |\n| `last_known_price` | float | √öltimo pre√ßo real conhecido |\n| `rmse` | float | Root Mean Square Error do modelo |\n| `training_samples` | integer | N√∫mero de sequ√™ncias usadas no treino |\n| `message` | string | Mensagem descritiva |\n\n#### Erros Comuns\n\n**400 - Dados Insuficientes**\n```json\n{\"detail\": \"M√≠nimo de 30 pontos hist√≥ricos necess√°rios. Fornecidos: 20\"}\n```\nSolu√ß√£o: Fornecer pelo menos 30 pontos de dados hist√≥ricos.\n\n**400 - Formato Inv√°lido**\n```json\n{\"detail\": \"Campo 'close' inv√°lido no ponto 5\"}\n```\nSolu√ß√£o: Verificar se todos os pontos t√™m date, close e volume v√°lidos.\n\n#### Casos de Uso do Endpoint Custom\n\n1. **Valida√ß√£o de Estrat√©gias**: Teste sua estrat√©gia de trading com dados hist√≥ricos espec√≠ficos\n2. **Testes com Dados Sint√©ticos**: Valide o modelo com dados gerados\n3. **An√°lise What-If**: \"E se os pre√ßos tivessem evolu√≠do diferente?\"\n4. **Backtesting**: Teste o modelo com per√≠odos hist√≥ricos espec√≠ficos\n\n#### Performance do Endpoint Custom\n\n| M√©trica | Valor T√≠pico |\n|---------|--------------|\n| Tempo (50 pontos) | 10-20 segundos |\n| Tempo (100 pontos) | 20-40 segundos |\n| Tempo (200 pontos) | 40-80 segundos |\n| Mem√≥ria utilizada | ~500 MB |\n\n## \uD83C\uDFAF Endpoints da API\n\n| M√©todo | Endpoint | Descri√ß√£o |\n|--------|----------|-----------|\n| GET | `/` | Informa√ß√µes da API |\n| GET | `/health` | Health check |\n| GET | `/predict/{ticker}` | Previs√£o para ticker (params: days, start_date, end_date) |\n| POST | `/predict` | Previs√£o com JSON completo |\n| POST | `/train` | Treinar modelo para ticker |\n| POST | `/predict-custom` | Treinar/prever com dados personalizados (isolado) |\n\n### Compara√ß√£o de Endpoints\n\n| Caracter√≠stica | `/predict/{ticker}` | `/train` | `/predict-custom` |\n|----------------|---------------------|----------|-------------------|\n| Usa dados do Yahoo Finance | ‚úÖ | ‚úÖ | ‚ùå |\n| Salva modelo | ‚ùå | ‚úÖ | ‚ùå |\n| Dados personalizados | ‚ùå | ‚ùå | ‚úÖ |\n| Cache de modelos | ‚úÖ | ‚úÖ | ‚ùå |\n| Retorna RMSE | ‚ùå | ‚úÖ | ‚úÖ |\n| Isolado | ‚ùå | ‚ùå | ‚úÖ |\n\n## \uD83C\uDFD7\uFE0F Arquitetura do Sistema\n\n### Componentes Principais\n\n**Pipeline ETL**\n- `yahoo_extractor.py` - Extra√ß√£o de dados do Yahoo Finance\n- `price_transformer.py` - Feature engineering (m√©dias m√≥veis, volatilidade, retornos)\n- `parquet_loader.py` - Armazenamento h√≠brido (local/S3)\n- `scrapper_pipeline.py` - Orquestra√ß√£o do pipeline\n\n**Machine Learning**\n- `model_training.py` - Treinamento LSTM com MLFlow\n- `model_executor.py` - Carregamento e infer√™ncia de modelos\n\n**API REST**\n- `api.py` - FastAPI com endpoints de previs√£o e treinamento\n\n### Fluxo de Dados\n\n```\nCliente ‚Üí API ‚Üí Verifica Cache Local ‚Üí Se n√£o existe ‚Üí Yahoo Finance\n                     ‚Üì                                        ‚Üì\n                 Model LSTM ‚Üê Dados Processados ‚Üê Feature Engineering\n                     ‚Üì\n                 Previs√£o ‚Üí Resposta JSON ‚Üí Cliente\n```\n\n### Modelo LSTM\n\n- **Arquitetura**: 2 camadas LSTM, 64 neur√¥nios por camada\n- **Features**: Pre√ßo de fechamento + Volume\n- **Janela temporal**: 30 dias\n- **Normaliza√ß√£o**: MinMaxScaler\n- **Armazenamento**: `export/lstm_model_{TICKER}.pth`\n\n## \uD83D\uDCE6 Estrutura do Projeto\n\n```\n‚îú‚îÄ‚îÄ api.py                          # API REST FastAPI\n‚îú‚îÄ‚îÄ model_executor.py               # Infer√™ncia de modelos\n‚îú‚îÄ‚îÄ requirements.txt                # Depend√™ncias Python\n‚îú‚îÄ‚îÄ docker-compose.yml              # Orquestra√ß√£o Docker\n‚îú‚îÄ‚îÄ model/\n‚îÇ   ‚îî‚îÄ‚îÄ model_training.py          # Treinamento LSTM + MLFlow\n‚îú‚îÄ‚îÄ scrapper/\n‚îÇ   ‚îú‚îÄ‚îÄ scrapper_pipeline.py       # Orquestra√ß√£o ETL\n‚îÇ   ‚îú‚îÄ‚îÄ scr/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extract/\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ yahoo_extractor.py # Extra√ß√£o Yahoo Finance\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transform/\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ price_transformer.py # Feature engineering\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ load/\n‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ parquet_loader.py  # Armazenamento local/S3\n‚îÇ   ‚îî‚îÄ‚îÄ data/                      # Dados locais (raw + processed)\n‚îî‚îÄ‚îÄ export/                        # Modelos treinados e scalers\n```\n\n## \uD83D\uDC33 Docker\n\n### Executar com Docker Compose (Recomendado)\n\n```bash\n# Iniciar\ndocker-compose up -d\n\n# Ver logs\ndocker-compose logs -f\n\n# Parar\ndocker-compose down\n```\n\n### Docker Standalone\n\n```bash\n# Build\ndocker build -t stock-api .\n\n# Run\ndocker run -d -p 8000:8000 stock-api\n```\n\n## \uD83D\uDCC8 MLFlow - Monitoramento de Experimentos\n\nO sistema usa MLFlow para rastrear todos os treinamentos.\n\n### Visualizar Experimentos\n\n```bash\n# Iniciar MLFlow UI\nmlflow ui\n\n# Acessar em: http://localhost:5000\n```\n\n**M√©tricas rastreadas:**\n- **Par√¢metros**: ticker, datas, √©pocas, learning rate, batch size\n- **M√©tricas**: RMSE, loss por √©poca, √∫ltima previs√£o\n- **Artefatos**: Modelos salvos (`.pth`), scalers (`.save`)\n- **Tags**: vers√£o, timestamp, dura√ß√£o do treinamento\n\n### Comparar Modelos\n\nNo MLFlow UI voc√™ pode:\n- Comparar RMSE entre diferentes tickers\n- Ver evolu√ß√£o do loss durante treinamento\n- Analisar distribui√ß√£o de previs√µes\n- Baixar modelos de vers√µes anteriores\n- Filtrar experimentos por par√¢metros\n- Exportar resultados para an√°lise\n\n## \uD83D\uDCA1 Exemplos Pr√°ticos\n\n### Exemplo 1: Treinar M√∫ltiplos Tickers\n\n```bash\n# Treinar PETR4\ncurl -X POST http://localhost:8000/train \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"ticker\": \"PETR4.SA\"}'\n\n# Treinar VALE3\ncurl -X POST http://localhost:8000/train \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"ticker\": \"VALE3.SA\"}'\n\n# Treinar ITUB4\ncurl -X POST http://localhost:8000/train \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"ticker\": \"ITUB4.SA\"}'\n```\n\n### Exemplo 2: Previs√µes para M√∫ltiplos Tickers\n\n```bash\n# PETR4 - 7 dias\ncurl \"http://localhost:8000/predict/PETR4.SA?days=7\"\n\n# VALE3 - 30 dias\ncurl \"http://localhost:8000/predict/VALE3.SA?days=30\"\n\n# ITUB4 - 14 dias\ncurl \"http://localhost:8000/predict/ITUB4.SA?days=14\"\n```\n\n### Exemplo 3: Dados Personalizados\n\n```bash\n# Criar arquivo com dados hist√≥ricos\ncat > custom_data.json << 'EOF'\n{\n  \"ticker_name\": \"TESTE_ACAO\",\n  \"days\": 7,\n  \"historical_data\": [\n    {\"date\": \"2024-01-01\", \"close\": 50.00, \"volume\": 1000000},\n    {\"date\": \"2024-01-02\", \"close\": 50.50, \"volume\": 1050000},\n    {\"date\": \"2024-01-03\", \"close\": 51.20, \"volume\": 1100000},\n    ... (m√≠nimo 30 pontos)\n  ]\n}\nEOF\n\n# Fazer previs√£o com dados personalizados\ncurl -X POST http://localhost:8000/predict-custom \\\n  -H \"Content-Type: application/json\" \\\n  -d @custom_data.json\n```\n\n**Vantagens do endpoint custom:**\n- N√£o precisa ter o ticker na bolsa\n- √ötil para testes com dados sint√©ticos\n- N√£o interfere com modelos salvos\n- Permite valida√ß√£o de estrat√©gias com dados hist√≥ricos espec√≠ficos\n\nVeja tamb√©m:\n- [example_custom_prediction.py](example_custom_prediction.py) - Script completo de exemplo\n- [example_custom_data.json](example_custom_data.json) - Dados de exemplo prontos (45 pontos hist√≥ricos)\n\n**Scripts inclu√≠dos:**\n```bash\n# Script completo com gera√ß√£o autom√°tica de dados\npython example_custom_prediction.py\n\n# Usar dados do arquivo JSON de exemplo\ncurl -X POST http://localhost:8000/predict-custom \\\n  -H \"Content-Type: application/json\" \\\n  -d @example_custom_data.json\n```\n\n### Exemplo 4: Treinamento Autom√°tico\n\n```bash\n# Execute o script de demonstra√ß√£o\npython example_auto_train.py\n```\n\n### Exemplo 5: Script Python para M√∫ltiplos Tickers\n\n```python\nimport requests\n\ntickers = ['PETR4.SA', 'VALE3.SA', 'ITUB4.SA']\nfor ticker in tickers:\n    r = requests.post('http://localhost:8000/train', \n                      json={'ticker': ticker})\n    print(f'{ticker}: {r.json()}')\n```\n\n## \uD83D\uDD27 Troubleshooting\n\n### Erro: \"Modelo n√£o encontrado\"\n- **Solu√ß√£o Autom√°tica**: O sistema agora treina automaticamente modelos inexistentes\n- **Manual**: Voc√™ ainda pode treinar explicitamente com `POST /train` se preferir\n\n### Erro: \"Dados insuficientes\"\n- **Solu√ß√£o**: Ajustar `start_date` para obter mais hist√≥rico (m√≠nimo 30 registros)\n\n### Erro: \"Ticker n√£o encontrado\"\n- **Solu√ß√£o**: Verificar se o ticker est√° correto (ex: PETR4.SA, n√£o PETR4)\n\n### Erro: \"M√≠nimo de 30 pontos hist√≥ricos necess√°rios\"\n- **Solu√ß√£o**: Para `/predict-custom`, fornecer pelo menos 30 pontos de dados hist√≥ricos\n- **Recomenda√ß√£o**: Use 60+ pontos para melhor acur√°cia do modelo\n\n### Treinamento Muito Lento (Endpoint Custom)\n- **Solu√ß√£o 1**: Reduza o n√∫mero de pontos hist√≥ricos\n- **Solu√ß√£o 2**: Verifique recursos do servidor (CPU/RAM)\n- **Solu√ß√£o 3**: Aumente o timeout do cliente (padr√£o: 300s)\n\n### RMSE Muito Alto\n- **Solu√ß√£o 1**: Verifique a qualidade dos dados\n- **Solu√ß√£o 2**: Aumente o n√∫mero de pontos hist√≥ricos\n- **Solu√ß√£o 3**: Verifique se h√° valores ausentes ou inconsistentes\n\n### Erro: ModuleNotFoundError\n- **Solu√ß√£o**: Verificar se todas as depend√™ncias foram instaladas com `pip install -r requirements.txt`\n\n### API n√£o responde\n- **Solu√ß√£o**: Verificar se a porta 8000 est√° dispon√≠vel e o servi√ßo est√° rodando\n\n### Ver Logs Detalhados\n\nO sistema possui logs detalhados em todos os processos:\n\n```python\nimport logging\n\n# Configurar n√≠vel de log\nlogging.basicConfig(\n    level=logging.DEBUG,  # Para logs muito detalhados\n    format=\"%(asctime)s | %(name)s | %(levelname)s | %(message)s\"\n)\n\n# Rodar a aplica√ß√£o\n# Voc√™ ver√° logs de:\n# - Carregamento de modelos\n# - Verifica√ß√£o de arquivos\n# - Treinamento autom√°tico\n# - Prepara√ß√£o de dados\n# - Previs√µes\n```\n\n## \uD83D\uDCCA Performance\n\n| M√©trica | Valor |\n|---------|-------|\n| Tempo com cache | 2-5 segundos |\n| Tempo sem cache (1¬™ vez) | 10-20 segundos |\n| Tamanho Docker image | ~2-3GB |\n| Mem√≥ria requerida | ~2GB RAM |\n\n## \uD83C\uDF93 Tickers Suportados\n\nQualquer ticker do Yahoo Finance (formato: CODIGO.SA para Brasil):\n\n- **Bancos**: ITUB4.SA, BBDC4.SA, SANB11.SA\n- **Energia**: PETR4.SA, ELET3.SA\n- **Minera√ß√£o**: VALE3.SA\n- **Varejo**: MGLU3.SA, LREN3.SA\n- **E muito mais...**\n\n## \uD83D\uDCDA Documenta√ß√£o Adicional\n\n- **Swagger UI** - http://localhost:8000/docs (documenta√ß√£o interativa completa)\n- **ReDoc** - http://localhost:8000/redoc (documenta√ß√£o alternativa)\n- **MLFlow UI** - http://localhost:5000 (ap√≥s executar `mlflow ui`)\n\n### Documenta√ß√£o da API\n\nA API possui documenta√ß√£o interativa autom√°tica gerada pelo FastAPI:\n\n1. **Swagger UI**: Interface interativa onde voc√™ pode testar todos os endpoints\n   - Acesse: http://localhost:8000/docs\n   - Recursos: Teste de endpoints, visualiza√ß√£o de schemas, exemplos\n\n2. **ReDoc**: Documenta√ß√£o alternativa em formato de p√°gina √∫nica\n   - Acesse: http://localhost:8000/redoc\n   - Recursos: Visualiza√ß√£o limpa, navega√ß√£o f√°cil, download de spec OpenAPI\n\n## \uD83D\uDCCA MLFlow - Rastreamento de Experimentos\n\nO MLFlow est√° **sempre habilitado** e rastreia automaticamente todas as opera√ß√µes de treinamento e previs√£o.\n\n### O que √© MLFlow?\n\nMLFlow √© uma plataforma open-source para gerenciar o ciclo de vida completo de Machine Learning. Neste projeto, ele rastreia:\n\n- ‚úÖ Par√¢metros de treinamento (√©pocas, learning rate, etc.)\n- ‚úÖ M√©tricas de performance (RMSE, acur√°cia)\n- ‚úÖ Modelos treinados (arquitetura e pesos)\n- ‚úÖ Artefatos (scalers, checkpoints)\n- ‚úÖ Previs√µes realizadas\n- ‚úÖ Dados de entrada (quantidade de pontos, datas)\n\n### Iniciando o MLFlow UI\n\n```bash\n# Iniciar interface web\nmlflow ui\n\n# Acesse: http://localhost:5000\n```\n\nOu em porta espec√≠fica:\n\n```bash\nmlflow ui --port 5001\n```\n\n### O que √© Rastreado Automaticamente\n\n#### Durante o Treinamento (`/train`)\n\n- **Par√¢metros**: ticker, start_date, end_date, seq_length, epochs, learning_rate, hidden_size, num_layers\n- **M√©tricas**: rmse (train), rmse (test), next_prediction, data_points\n- **Artefatos**: lstm_model_{ticker}.pth, scaler_features_{ticker}.save, scaler_close_{ticker}.save\n\n#### Durante Previs√µes (`/predict`)\n\n- **Par√¢metros**: ticker, days, endpoint, start_date, end_date\n- **M√©tricas**: data_points, last_known_price, prediction_day_1, prediction_day_2, ...\n\n#### Durante Previs√µes Customizadas (`/predict-custom`)\n\n- **Par√¢metros**: ticker_name, days, seq_length, epochs, learning_rate\n- **M√©tricas**: historical_data_points, train_samples, test_samples, rmse, previs√µes\n\n### Visualizando Experimentos\n\nAp√≥s executar `mlflow ui`, voc√™ ver√°:\n\n- **Runs**: Lista de todas as execu√ß√µes\n- **Parameters**: Hiperpar√¢metros de cada run\n- **Metrics**: Gr√°ficos de m√©tricas ao longo do tempo\n- **Artifacts**: Modelos e arquivos salvos\n- **Comparison**: Comparar m√∫ltiplas execu√ß√µes\n\n### Exemplos de Uso do MLFlow\n\n#### 1. Treinar e Visualizar\n\n```bash\n# Treinar modelo\ncurl -X POST http://localhost:8000/train \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"ticker\": \"PETR4.SA\"}'\n\n# Visualizar no MLFlow\nmlflow ui\n# Abrir http://localhost:5000\n# Ver experimento \"stock-price-prediction\"\n# Verificar m√©tricas de RMSE\n```\n\n#### 2. Comparar Diferentes Tickers\n\n```bash\n# Treinar m√∫ltiplos tickers\ncurl -X POST http://localhost:8000/train -d '{\"ticker\": \"PETR4.SA\"}'\ncurl -X POST http://localhost:8000/train -d '{\"ticker\": \"VALE3.SA\"}'\ncurl -X POST http://localhost:8000/train -d '{\"ticker\": \"ITUB4.SA\"}'\n\n# No MLFlow UI:\n# - Filtrar por ticker\n# - Comparar RMSE\n# - Identificar melhor performance\n```\n\n#### 3. Filtrando Resultados\n\nNo MLFlow UI, use filtros como:\n\n```python\n# Filtrar por ticker\nticker = \"VALE3.SA\"\n\n# Filtrar por RMSE baixo\nmetrics.rmse < 5.0\n\n# Filtrar por data\nattributes.start_time > \"2026-01-01\"\n```\n\n### M√©tricas Importantes\n\n#### RMSE (Root Mean Square Error)\n\nMede o erro m√©dio das previs√µes:\n\n- **RMSE < 2**: Excelente\n- **RMSE 2-5**: Bom\n- **RMSE 5-10**: Aceit√°vel\n- **RMSE > 10**: Necessita ajustes\n\n### Configura√ß√£o Avan√ßada\n\n#### Servidor Remoto\n\n```bash\n# .env\nMLFLOW_TRACKING_URI=http://mlflow-server:5000\nMLFLOW_EXPERIMENT_NAME=stock-production\n```\n\n#### PostgreSQL Backend\n\n```bash\n# .env\nMLFLOW_TRACKING_URI=postgresql://user:password@localhost/mlflow\n```\n\n#### S3 para Artefatos\n\n```bash\n# .env\nMLFLOW_TRACKING_URI=http://mlflow-server:5000\nMLFLOW_S3_ENDPOINT_URL=https://s3.amazonaws.com\nAWS_ACCESS_KEY_ID=sua-chave\nAWS_SECRET_ACCESS_KEY=seu-secret\n```\n\n### Estrutura de Dados do MLFlow\n\n```\nmlruns/\n‚îú‚îÄ‚îÄ 0/                          # Experimento padr√£o\n‚îÇ   ‚îú‚îÄ‚îÄ meta.yaml\n‚îÇ   ‚îî‚îÄ‚îÄ <run-id>/               # Cada execu√ß√£o tem um ID √∫nico\n‚îÇ       ‚îú‚îÄ‚îÄ artifacts/          # Modelos, scalers salvos\n‚îÇ       ‚îú‚îÄ‚îÄ metrics/            # RMSE, predictions, etc.\n‚îÇ       ‚îú‚îÄ‚îÄ params/             # Hiperpar√¢metros\n‚îÇ       ‚îî‚îÄ‚îÄ tags/               # Tags customizadas\n‚îî‚îÄ‚îÄ <experiment-id>/            # Experimento \"stock-price-prediction\"\n    ‚îî‚îÄ‚îÄ ...\n```\n\n### Boas Pr√°ticas\n\n1. **Nomear Experimentos**:\n   - Produ√ß√£o: `MLFLOW_EXPERIMENT_NAME=stock-production`\n   - Desenvolvimento: `MLFLOW_EXPERIMENT_NAME=stock-dev`\n   - Testes: `MLFLOW_EXPERIMENT_NAME=stock-experiments`\n\n2. **Tags Customizadas**:\n   ```python\n   mlflow.set_tag(\"environment\", \"production\")\n   mlflow.set_tag(\"model_version\", \"v2.0\")\n   mlflow.set_tag(\"data_source\", \"yahoo_finance\")\n   ```\n\n3. **Backup de Experimentos**:\n   ```bash\n   # Exportar\n   mlflow experiments export --experiment-id 0 --output-dir backup/\n   \n   # Importar\n   mlflow experiments import --input-dir backup/\n   ```\n\n### Troubleshooting\n\n#### MLFlow UI n√£o inicia\n\n```bash\n# Verificar porta ocupada (Windows)\nnetstat -ano | findstr :5000\n\n# Usar outra porta\nmlflow ui --port 5001\n```\n\n#### Experimentos n√£o aparecem\n\n```bash\n# Verificar diret√≥rio mlruns\ndir mlruns\n\n# Resetar para local\nmlflow ui\n```\n\n## ‚öô\uFE0F Arquivos de Exemplo\n\n- **example_auto_train.py** - Demonstra√ß√£o de treinamento autom√°tico\n- **example_custom_prediction.py** - Exemplo completo de uso do endpoint custom\n- **example_custom_data.json** - Dados de exemplo prontos para uso\n- **test_system.py** - Script de teste completo do sistema\n- **example_mlflow.py** - Demonstra√ß√£o do uso do MLFlow\n\n## \uD83D\uDD17 Links √öteis\n\n- [FastAPI](https://fastapi.tiangolo.com/)\n- [PyTorch](https://pytorch.org/)\n- [MLFlow](https://mlflow.org/)\n- [Yahoo Finance](https://finance.yahoo.com/)\n\n## \uD83D\uDCDD Notas Importantes\n\n1. ‚úÖ **Treinamento autom√°tico** - O sistema detecta automaticamente quando um modelo n√£o existe e treina sob demanda\n2. \uD83D\uDCE6 **Modelos independentes** - Cada ticker tem seu pr√≥prio modelo (`export/lstm_model_{TICKER}.pth`)\n3. \uD83D\uDCBE **Cache local** - Dados s√£o salvos localmente para evitar downloads repetidos do Yahoo Finance\n4. \uD83D\uDD0D **Logs detalhados** - Todo o processo √© logado para f√°cil debugging\n5. ‚öô\uFE0F **Configura√ß√£o flex√≠vel** - Use vari√°veis de ambiente para alternar entre local/S3\n6. \uD83D\uDE80 **Pronto para produ√ß√£o** - Suporte completo para Docker e MLFlow\n7. ‚è±\uFE0F **Tempo de treinamento** - O treinamento leva geralmente 2-5 minutos por ticker\n\n## \uD83D\uDD04 Fluxo de Trabalho Recomendado\n\n### Para um Novo Ticker\n\n1. **Simplesmente fa√ßa a previs√£o**:\n   ```bash\n   curl \"http://localhost:8000/predict/NOVO_TICKER.SA?days=7\"\n   ```\n   O sistema ir√° automaticamente:\n   - Buscar dados do Yahoo Finance\n   - Treinar o modelo\n   - Fazer a previs√£o\n\n2. **Ou treine explicitamente** (opcional):\n   ```bash\n   curl -X POST http://localhost:8000/train \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"ticker\": \"NOVO_TICKER.SA\"}'\n   ```\n\n### Para Retreinar um Ticker Existente\n\nSimplesmente execute o treinamento novamente. O novo modelo substituir√° o anterior:\n\n```bash\ncurl -X POST http://localhost:8000/train \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"ticker\": \"PETR4.SA\", \"start_date\": \"2020-01-01\"}'\n```\n\n---\n\n**Desenvolvido para FIAP - Sistema de Previs√£o de A√ß√µes com Machine Learning**
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/README.md b/README.md
--- a/README.md	(revision 17f2ab735aa08aa0cd60b304b1c1940161aa971d)
+++ b/README.md	(date 1767837023322)
@@ -1,13 +1,13 @@
 # API de Previs√£o de Pre√ßos de A√ß√µes
 
-Sistema completo de previs√£o de pre√ßos de a√ß√µes usando modelo LSTM (Long Short-Term Memory) com pipeline ETL automatizado, API REST e **monitoramento MLFlow sempre habilitado**.
+Sistema completo de previs√£o de pre√ßos de a√ß√µes usando modelo LSTM (Long Short-Term Memory) com pipeline ETL automatizado, API REST e monitoramento MLFlow.
 
 ## üöÄ Caracter√≠sticas Principais
 
 - ‚úÖ **Multi-ticker**: Suporta qualquer a√ß√£o da bolsa brasileira
 - ‚úÖ **Treinamento autom√°tico**: Treina modelos sob demanda quando necess√°rio
 - ‚úÖ **Armazenamento h√≠brido**: Local ou S3 via vari√°vel de ambiente
-- ‚úÖ **MLFlow sempre ativo**: Rastreamento autom√°tico de todas as opera√ß√µes
+- ‚úÖ **MLFlow**: Monitoramento completo de experimentos
 - ‚úÖ **Dados personalizados**: Endpoint para treinar/prever com seus pr√≥prios dados
 - ‚úÖ **Logs detalhados**: Rastreamento completo de opera√ß√µes
 - ‚úÖ **Cache inteligente**: Modelos permanecem em mem√≥ria ap√≥s carregamento
@@ -51,22 +51,6 @@
 
 Acesse: http://localhost:8000/docs
 
-### 4. Visualizar Experimentos MLFlow (Opcional)
-
-```bash
-mlflow ui
-```
-
-Acesse: http://localhost:5000
-
-O MLFlow rastreia automaticamente:
-- Par√¢metros de treinamento
-- M√©tricas (RMSE, previs√µes)
-- Modelos e artefatos
-- Hist√≥rico completo de opera√ß√µes
-
-üìñ **Guia completo**: [MLFLOW_GUIDE.md](MLFLOW_GUIDE.md)
-
 ## üìä Como Usar
 
 ### Treinar um Modelo
@@ -650,215 +634,12 @@
    - Acesse: http://localhost:8000/redoc
    - Recursos: Visualiza√ß√£o limpa, navega√ß√£o f√°cil, download de spec OpenAPI
 
-## üìä MLFlow - Rastreamento de Experimentos
-
-O MLFlow est√° **sempre habilitado** e rastreia automaticamente todas as opera√ß√µes de treinamento e previs√£o.
-
-### O que √© MLFlow?
-
-MLFlow √© uma plataforma open-source para gerenciar o ciclo de vida completo de Machine Learning. Neste projeto, ele rastreia:
-
-- ‚úÖ Par√¢metros de treinamento (√©pocas, learning rate, etc.)
-- ‚úÖ M√©tricas de performance (RMSE, acur√°cia)
-- ‚úÖ Modelos treinados (arquitetura e pesos)
-- ‚úÖ Artefatos (scalers, checkpoints)
-- ‚úÖ Previs√µes realizadas
-- ‚úÖ Dados de entrada (quantidade de pontos, datas)
-
-### Iniciando o MLFlow UI
-
-```bash
-# Iniciar interface web
-mlflow ui
-
-# Acesse: http://localhost:5000
-```
-
-Ou em porta espec√≠fica:
-
-```bash
-mlflow ui --port 5001
-```
-
-### O que √© Rastreado Automaticamente
-
-#### Durante o Treinamento (`/train`)
-
-- **Par√¢metros**: ticker, start_date, end_date, seq_length, epochs, learning_rate, hidden_size, num_layers
-- **M√©tricas**: rmse (train), rmse (test), next_prediction, data_points
-- **Artefatos**: lstm_model_{ticker}.pth, scaler_features_{ticker}.save, scaler_close_{ticker}.save
-
-#### Durante Previs√µes (`/predict`)
-
-- **Par√¢metros**: ticker, days, endpoint, start_date, end_date
-- **M√©tricas**: data_points, last_known_price, prediction_day_1, prediction_day_2, ...
-
-#### Durante Previs√µes Customizadas (`/predict-custom`)
-
-- **Par√¢metros**: ticker_name, days, seq_length, epochs, learning_rate
-- **M√©tricas**: historical_data_points, train_samples, test_samples, rmse, previs√µes
-
-### Visualizando Experimentos
-
-Ap√≥s executar `mlflow ui`, voc√™ ver√°:
-
-- **Runs**: Lista de todas as execu√ß√µes
-- **Parameters**: Hiperpar√¢metros de cada run
-- **Metrics**: Gr√°ficos de m√©tricas ao longo do tempo
-- **Artifacts**: Modelos e arquivos salvos
-- **Comparison**: Comparar m√∫ltiplas execu√ß√µes
-
-### Exemplos de Uso do MLFlow
-
-#### 1. Treinar e Visualizar
-
-```bash
-# Treinar modelo
-curl -X POST http://localhost:8000/train \
-  -H "Content-Type: application/json" \
-  -d '{"ticker": "PETR4.SA"}'
-
-# Visualizar no MLFlow
-mlflow ui
-# Abrir http://localhost:5000
-# Ver experimento "stock-price-prediction"
-# Verificar m√©tricas de RMSE
-```
-
-#### 2. Comparar Diferentes Tickers
-
-```bash
-# Treinar m√∫ltiplos tickers
-curl -X POST http://localhost:8000/train -d '{"ticker": "PETR4.SA"}'
-curl -X POST http://localhost:8000/train -d '{"ticker": "VALE3.SA"}'
-curl -X POST http://localhost:8000/train -d '{"ticker": "ITUB4.SA"}'
-
-# No MLFlow UI:
-# - Filtrar por ticker
-# - Comparar RMSE
-# - Identificar melhor performance
-```
-
-#### 3. Filtrando Resultados
-
-No MLFlow UI, use filtros como:
-
-```python
-# Filtrar por ticker
-ticker = "VALE3.SA"
-
-# Filtrar por RMSE baixo
-metrics.rmse < 5.0
-
-# Filtrar por data
-attributes.start_time > "2026-01-01"
-```
-
-### M√©tricas Importantes
-
-#### RMSE (Root Mean Square Error)
-
-Mede o erro m√©dio das previs√µes:
-
-- **RMSE < 2**: Excelente
-- **RMSE 2-5**: Bom
-- **RMSE 5-10**: Aceit√°vel
-- **RMSE > 10**: Necessita ajustes
-
-### Configura√ß√£o Avan√ßada
-
-#### Servidor Remoto
-
-```bash
-# .env
-MLFLOW_TRACKING_URI=http://mlflow-server:5000
-MLFLOW_EXPERIMENT_NAME=stock-production
-```
-
-#### PostgreSQL Backend
-
-```bash
-# .env
-MLFLOW_TRACKING_URI=postgresql://user:password@localhost/mlflow
-```
-
-#### S3 para Artefatos
-
-```bash
-# .env
-MLFLOW_TRACKING_URI=http://mlflow-server:5000
-MLFLOW_S3_ENDPOINT_URL=https://s3.amazonaws.com
-AWS_ACCESS_KEY_ID=sua-chave
-AWS_SECRET_ACCESS_KEY=seu-secret
-```
-
-### Estrutura de Dados do MLFlow
-
-```
-mlruns/
-‚îú‚îÄ‚îÄ 0/                          # Experimento padr√£o
-‚îÇ   ‚îú‚îÄ‚îÄ meta.yaml
-‚îÇ   ‚îî‚îÄ‚îÄ <run-id>/               # Cada execu√ß√£o tem um ID √∫nico
-‚îÇ       ‚îú‚îÄ‚îÄ artifacts/          # Modelos, scalers salvos
-‚îÇ       ‚îú‚îÄ‚îÄ metrics/            # RMSE, predictions, etc.
-‚îÇ       ‚îú‚îÄ‚îÄ params/             # Hiperpar√¢metros
-‚îÇ       ‚îî‚îÄ‚îÄ tags/               # Tags customizadas
-‚îî‚îÄ‚îÄ <experiment-id>/            # Experimento "stock-price-prediction"
-    ‚îî‚îÄ‚îÄ ...
-```
-
-### Boas Pr√°ticas
-
-1. **Nomear Experimentos**:
-   - Produ√ß√£o: `MLFLOW_EXPERIMENT_NAME=stock-production`
-   - Desenvolvimento: `MLFLOW_EXPERIMENT_NAME=stock-dev`
-   - Testes: `MLFLOW_EXPERIMENT_NAME=stock-experiments`
-
-2. **Tags Customizadas**:
-   ```python
-   mlflow.set_tag("environment", "production")
-   mlflow.set_tag("model_version", "v2.0")
-   mlflow.set_tag("data_source", "yahoo_finance")
-   ```
-
-3. **Backup de Experimentos**:
-   ```bash
-   # Exportar
-   mlflow experiments export --experiment-id 0 --output-dir backup/
-   
-   # Importar
-   mlflow experiments import --input-dir backup/
-   ```
-
-### Troubleshooting
-
-#### MLFlow UI n√£o inicia
-
-```bash
-# Verificar porta ocupada (Windows)
-netstat -ano | findstr :5000
-
-# Usar outra porta
-mlflow ui --port 5001
-```
-
-#### Experimentos n√£o aparecem
-
-```bash
-# Verificar diret√≥rio mlruns
-dir mlruns
-
-# Resetar para local
-mlflow ui
-```
-
 ## ‚öôÔ∏è Arquivos de Exemplo
 
 - **example_auto_train.py** - Demonstra√ß√£o de treinamento autom√°tico
 - **example_custom_prediction.py** - Exemplo completo de uso do endpoint custom
 - **example_custom_data.json** - Dados de exemplo prontos para uso
 - **test_system.py** - Script de teste completo do sistema
-- **example_mlflow.py** - Demonstra√ß√£o do uso do MLFlow
 
 ## üîó Links √öteis
 
Index: MLFLOW_GUIDE.md
===================================================================
diff --git a/MLFLOW_GUIDE.md b/MLFLOW_GUIDE.md
deleted file mode 100644
--- a/MLFLOW_GUIDE.md	(revision 17f2ab735aa08aa0cd60b304b1c1940161aa971d)
+++ /dev/null	(revision 17f2ab735aa08aa0cd60b304b1c1940161aa971d)
@@ -1,346 +0,0 @@
-# Guia MLFlow - Sistema de Previs√£o de A√ß√µes
-
-## üìä O que √© MLFlow?
-
-MLFlow √© uma plataforma open-source para gerenciar o ciclo de vida completo de Machine Learning. Neste projeto, ele est√° **sempre habilitado** e rastreia automaticamente:
-
-- ‚úÖ Par√¢metros de treinamento (√©pocas, learning rate, etc.)
-- ‚úÖ M√©tricas de performance (RMSE, acur√°cia)
-- ‚úÖ Modelos treinados (arquitetura e pesos)
-- ‚úÖ Artefatos (scalers, checkpoints)
-- ‚úÖ Previs√µes realizadas
-- ‚úÖ Dados de entrada (quantidade de pontos, datas)
-
-## üöÄ Iniciando o MLFlow UI
-
-### 1. Visualizar Experimentos Localmente
-
-```bash
-mlflow ui
-```
-
-Acesse: http://localhost:5000
-
-### 2. Visualizar Experimentos em Porta Espec√≠fica
-
-```bash
-mlflow ui --port 5001
-```
-
-Acesse: http://localhost:5001
-
-### 3. MLFlow UI com Backend Remoto
-
-Se voc√™ configurou um servidor MLFlow remoto:
-
-```bash
-# No .env
-MLFLOW_TRACKING_URI=http://seu-servidor-mlflow:5000
-
-# Iniciar UI
-mlflow ui --backend-store-uri http://seu-servidor-mlflow:5000
-```
-
-## üìÅ Estrutura de Dados do MLFlow
-
-```
-mlruns/
-‚îú‚îÄ‚îÄ 0/                          # Experimento padr√£o
-‚îÇ   ‚îú‚îÄ‚îÄ meta.yaml
-‚îÇ   ‚îî‚îÄ‚îÄ <run-id>/               # Cada execu√ß√£o tem um ID √∫nico
-‚îÇ       ‚îú‚îÄ‚îÄ artifacts/          # Modelos, scalers salvos
-‚îÇ       ‚îú‚îÄ‚îÄ metrics/            # RMSE, predictions, etc.
-‚îÇ       ‚îú‚îÄ‚îÄ params/             # Hiperpar√¢metros
-‚îÇ       ‚îî‚îÄ‚îÄ tags/               # Tags customizadas
-‚îî‚îÄ‚îÄ <experiment-id>/            # Experimento "stock-price-prediction"
-    ‚îî‚îÄ‚îÄ ...
-```
-
-## üîç O que √© Rastreado Automaticamente
-
-### Durante o Treinamento (`/train`)
-
-```python
-# Par√¢metros
-- ticker
-- start_date
-- end_date
-- seq_length: 30
-- epochs: 50
-- learning_rate: 0.001
-- hidden_size: 64
-- num_layers: 2
-
-# M√©tricas
-- rmse (train)
-- rmse (test)
-- next_prediction
-- data_points
-
-# Artefatos
-- lstm_model_{ticker}.pth
-- scaler_features_{ticker}.save
-- scaler_close_{ticker}.save
-```
-
-### Durante Previs√µes (`/predict`)
-
-```python
-# Par√¢metros
-- ticker
-- days
-- endpoint (GET ou POST)
-- start_date (opcional)
-- end_date (opcional)
-
-# M√©tricas
-- data_points
-- last_known_price
-- prediction_day_1
-- prediction_day_2
-- ...
-- prediction_day_N
-```
-
-### Durante Previs√µes Customizadas (`/predict-custom`)
-
-```python
-# Par√¢metros
-- ticker_name
-- days
-- endpoint
-- seq_length
-- epochs
-- learning_rate
-
-# M√©tricas
-- historical_data_points
-- train_samples
-- test_samples
-- rmse
-- last_known_price
-- prediction_day_1, prediction_day_2, ...
-```
-
-## üìä Visualizando Experimentos
-
-### 1. Interface Web
-
-Ap√≥s executar `mlflow ui`, voc√™ ver√°:
-
-- **Runs**: Lista de todas as execu√ß√µes
-- **Parameters**: Hiperpar√¢metros de cada run
-- **Metrics**: Gr√°ficos de m√©tricas ao longo do tempo
-- **Artifacts**: Modelos e arquivos salvos
-- **Comparison**: Comparar m√∫ltiplas execu√ß√µes
-
-### 2. Filtrando Resultados
-
-```python
-# Filtrar por ticker
-ticker = "VALE3.SA"
-
-# Filtrar por RMSE baixo
-metrics.rmse < 5.0
-
-# Filtrar por data
-attributes.start_time > "2026-01-01"
-```
-
-### 3. Comparando Modelos
-
-No MLFlow UI:
-1. Selecione m√∫ltiplas runs (checkbox)
-2. Clique em "Compare"
-3. Visualize gr√°ficos lado a lado
-4. Identifique o melhor modelo
-
-## üîß Configura√ß√£o Avan√ßada
-
-### Usar MLFlow com Servidor Remoto
-
-```bash
-# .env
-MLFLOW_TRACKING_URI=http://mlflow-server:5000
-MLFLOW_EXPERIMENT_NAME=stock-production
-```
-
-### Usar MLFlow com PostgreSQL
-
-```bash
-# .env
-MLFLOW_TRACKING_URI=postgresql://user:password@localhost/mlflow
-```
-
-### Usar MLFlow com S3 para Artefatos
-
-```bash
-# .env
-MLFLOW_TRACKING_URI=http://mlflow-server:5000
-MLFLOW_S3_ENDPOINT_URL=https://s3.amazonaws.com
-AWS_ACCESS_KEY_ID=sua-chave
-AWS_SECRET_ACCESS_KEY=seu-secret
-```
-
-## üìà M√©tricas Importantes
-
-### RMSE (Root Mean Square Error)
-
-Mede o erro m√©dio das previs√µes:
-
-- **RMSE < 2**: Excelente
-- **RMSE 2-5**: Bom
-- **RMSE 5-10**: Aceit√°vel
-- **RMSE > 10**: Necessita ajustes
-
-### Compara√ß√£o de Previs√µes
-
-O MLFlow permite visualizar:
-- Previs√µes vs. Valores Reais
-- Tend√™ncia ao longo do tempo
-- Acur√°cia por ticker
-- Performance por per√≠odo
-
-## üéØ Exemplos de Uso
-
-### 1. Treinar e Visualizar
-
-```bash
-# Treinar modelo
-curl -X POST http://localhost:8000/train \
-  -H "Content-Type: application/json" \
-  -d '{"ticker": "PETR4.SA"}'
-
-# Visualizar no MLFlow
-mlflow ui
-# Abrir http://localhost:5000
-# Ver experimento "stock-price-prediction"
-# Verificar m√©tricas de RMSE
-```
-
-### 2. Comparar Diferentes Tickers
-
-```bash
-# Treinar m√∫ltiplos tickers
-curl -X POST http://localhost:8000/train -d '{"ticker": "PETR4.SA"}'
-curl -X POST http://localhost:8000/train -d '{"ticker": "VALE3.SA"}'
-curl -X POST http://localhost:8000/train -d '{"ticker": "ITUB4.SA"}'
-
-# No MLFlow UI:
-# - Filtrar por ticker
-# - Comparar RMSE
-# - Identificar melhor performance
-```
-
-### 3. Rastrear Previs√µes ao Longo do Tempo
-
-```bash
-# Fazer previs√µes di√°rias
-curl "http://localhost:8000/predict/PETR4.SA?days=1"
-
-# No MLFlow:
-# - Ver hist√≥rico de previs√µes
-# - Comparar com pre√ßos reais
-# - Avaliar acur√°cia temporal
-```
-
-## üîí Boas Pr√°ticas
-
-### 1. Nomear Experimentos
-
-```python
-# Produ√ß√£o
-MLFLOW_EXPERIMENT_NAME=stock-production
-
-# Desenvolvimento
-MLFLOW_EXPERIMENT_NAME=stock-dev
-
-# Testes
-MLFLOW_EXPERIMENT_NAME=stock-experiments
-```
-
-### 2. Tags Customizadas
-
-```python
-# Em model_training.py ou api.py
-mlflow.set_tag("environment", "production")
-mlflow.set_tag("model_version", "v2.0")
-mlflow.set_tag("data_source", "yahoo_finance")
-```
-
-### 3. Backup de Experimentos
-
-```bash
-# Exportar experimentos
-mlflow experiments export --experiment-id 0 --output-dir backup/
-
-# Importar experimentos
-mlflow experiments import --input-dir backup/
-```
-
-## üêõ Troubleshooting
-
-### MLFlow UI n√£o inicia
-
-```bash
-# Verificar se a porta est√° ocupada
-lsof -i :5000  # Linux/Mac
-netstat -ano | findstr :5000  # Windows
-
-# Usar outra porta
-mlflow ui --port 5001
-```
-
-### Experimentos n√£o aparecem
-
-```bash
-# Verificar diret√≥rio mlruns
-ls -la mlruns/
-
-# Verificar vari√°vel de ambiente
-echo $MLFLOW_TRACKING_URI
-
-# Resetar para local
-unset MLFLOW_TRACKING_URI
-mlflow ui
-```
-
-### Erro ao salvar artefatos
-
-```bash
-# Verificar permiss√µes
-chmod -R 755 mlruns/
-
-# Verificar espa√ßo em disco
-df -h
-```
-
-## üìö Recursos Adicionais
-
-- [MLFlow Documentation](https://mlflow.org/docs/latest/index.html)
-- [MLFlow Tracking](https://mlflow.org/docs/latest/tracking.html)
-- [MLFlow Models](https://mlflow.org/docs/latest/models.html)
-- [MLFlow Projects](https://mlflow.org/docs/latest/projects.html)
-
-## üéì Conclus√£o
-
-O MLFlow est√° totalmente integrado ao sistema de previs√£o de a√ß√µes:
-
-1. ‚úÖ **Sempre habilitado** - Rastreamento autom√°tico em todas as opera√ß√µes
-2. ‚úÖ **Completo** - Rastreia treinamento, previs√µes e m√©tricas
-3. ‚úÖ **Transparente** - N√£o interfere no funcionamento da API
-4. ‚úÖ **Valioso** - Facilita compara√ß√µes e melhorias do modelo
-
-Use o MLFlow para:
-- Comparar performance entre tickers
-- Otimizar hiperpar√¢metros
-- Rastrear previs√µes ao longo do tempo
-- Identificar modelos que precisam retreinamento
-- Documentar experimentos
-
-**Comando principal:**
-```bash
-mlflow ui
-```
-
-Acesse http://localhost:5000 e explore seus experimentos! üöÄ
Index: example_mlflow.py
===================================================================
diff --git a/example_mlflow.py b/example_mlflow.py
deleted file mode 100644
--- a/example_mlflow.py	(revision 17f2ab735aa08aa0cd60b304b1c1940161aa971d)
+++ /dev/null	(revision 17f2ab735aa08aa0cd60b304b1c1940161aa971d)
@@ -1,165 +0,0 @@
-"""
-Exemplo de uso do MLFlow com o sistema de previs√£o de a√ß√µes
-
-Este script demonstra como o MLFlow rastreia automaticamente
-todas as opera√ß√µes de treinamento e previs√£o.
-"""
-
-import requests
-import time
-import json
-
-BASE_URL = "http://localhost:8000"
-
-
-def print_section(title):
-    """Imprime cabe√ßalho de se√ß√£o"""
-    print("\n" + "=" * 60)
-    print(f" {title}")
-    print("=" * 60 + "\n")
-
-
-def train_model(ticker, start_date="2020-01-01"):
-    """Treina um modelo e rastreia no MLFlow"""
-    print_section(f"Treinando modelo para {ticker}")
-    
-    response = requests.post(
-        f"{BASE_URL}/train",
-        json={
-            "ticker": ticker,
-            "start_date": start_date
-        }
-    )
-    
-    if response.status_code == 200:
-        result = response.json()
-        print(f"‚úÖ Treinamento conclu√≠do!")
-        print(f"   RMSE: {result.get('rmse', 'N/A')}")
-        print(f"   Pr√≥xima previs√£o: R$ {result.get('next_prediction', 'N/A'):.2f}")
-        print(f"   Treinado em: {result.get('trained_at', 'N/A')}")
-        print(f"\nüìä Veja os detalhes no MLFlow UI: http://localhost:5000")
-        return result
-    else:
-        print(f"‚ùå Erro: {response.status_code}")
-        print(f"   {response.text}")
-        return None
-
-
-def make_prediction(ticker, days=5):
-    """Faz uma previs√£o e rastreia no MLFlow"""
-    print_section(f"Fazendo previs√£o para {ticker} - {days} dias")
-    
-    response = requests.get(f"{BASE_URL}/predict/{ticker}?days={days}")
-    
-    if response.status_code == 200:
-        result = response.json()
-        print(f"‚úÖ Previs√£o conclu√≠da!")
-        print(f"   √öltimo pre√ßo conhecido: R$ {result['last_known_price']:.2f}")
-        print(f"   Previs√µes para {days} dias:")
-        
-        for i, pred in enumerate(result['predictions'], 1):
-            print(f"     Dia {i}: R$ {pred:.2f}")
-        
-        print(f"\nüìä Veja os detalhes no MLFlow UI: http://localhost:5000")
-        return result
-    else:
-        print(f"‚ùå Erro: {response.status_code}")
-        print(f"   {response.text}")
-        return None
-
-
-def predict_with_custom_data():
-    """Faz previs√£o com dados personalizados e rastreia no MLFlow"""
-    print_section("Previs√£o com dados personalizados")
-    
-    # Dados de exemplo
-    custom_data = {
-        "ticker_name": "TESTE_MLFLOW",
-        "days": 3,
-        "historical_data": [
-            {"date": f"2024-01-{str(i).zfill(2)}", "close": 100.0 + i * 0.5, "volume": 1000000 + i * 10000}
-            for i in range(1, 46)
-        ]
-    }
-    
-    response = requests.post(
-        f"{BASE_URL}/predict-custom",
-        json=custom_data
-    )
-    
-    if response.status_code == 200:
-        result = response.json()
-        print(f"‚úÖ Previs√£o customizada conclu√≠da!")
-        print(f"   Nome do ticker: {result['ticker_name']}")
-        print(f"   RMSE: {result['rmse']}")
-        print(f"   Amostras de treino: {result['training_samples']}")
-        print(f"   √öltimo pre√ßo conhecido: R$ {result['last_known_price']:.2f}")
-        print(f"   Previs√µes:")
-        
-        for i, pred in enumerate(result['predictions'], 1):
-            print(f"     Dia {i}: R$ {pred:.2f}")
-        
-        print(f"\nüìä Veja os detalhes no MLFlow UI: http://localhost:5000")
-        return result
-    else:
-        print(f"‚ùå Erro: {response.status_code}")
-        print(f"   {response.text}")
-        return None
-
-
-def main():
-    """Fun√ß√£o principal que demonstra o uso do MLFlow"""
-    
-    print("\nüöÄ Demonstra√ß√£o do MLFlow com Sistema de Previs√£o de A√ß√µes")
-    print("=" * 60)
-    print("\nEste script vai:")
-    print("1. Treinar um modelo para PETR4.SA")
-    print("2. Fazer previs√µes para 5 dias")
-    print("3. Fazer previs√£o com dados customizados")
-    print("\nTodas as opera√ß√µes ser√£o rastreadas automaticamente no MLFlow!")
-    print("\nüí° Dica: Abra http://localhost:5000 para ver os experimentos")
-    print("\nPressione Enter para continuar...")
-    input()
-    
-    # 1. Treinar modelo
-    ticker = "PETR4.SA"
-    train_result = train_model(ticker)
-    
-    if train_result:
-        print("\n‚è≥ Aguardando 2 segundos...")
-        time.sleep(2)
-        
-        # 2. Fazer previs√£o
-        predict_result = make_prediction(ticker, days=5)
-        
-        if predict_result:
-            print("\n‚è≥ Aguardando 2 segundos...")
-            time.sleep(2)
-            
-            # 3. Previs√£o customizada
-            custom_result = predict_with_custom_data()
-    
-    # Resumo final
-    print_section("Resumo da Demonstra√ß√£o")
-    print("‚úÖ Demonstra√ß√£o conclu√≠da!")
-    print("\nüìä Pr√≥ximos passos:")
-    print("   1. Abra http://localhost:5000 (MLFlow UI)")
-    print("   2. Veja o experimento 'stock-price-prediction'")
-    print("   3. Compare as m√©tricas (RMSE, previs√µes)")
-    print("   4. Explore os par√¢metros e artefatos")
-    print("\nüìñ Guia completo: MLFLOW_GUIDE.md")
-    print("\nüéØ O MLFlow est√° sempre ativo e rastreando todas as opera√ß√µes!")
-    print("=" * 60 + "\n")
-
-
-if __name__ == "__main__":
-    try:
-        main()
-    except KeyboardInterrupt:
-        print("\n\n‚ùå Opera√ß√£o cancelada pelo usu√°rio")
-    except requests.exceptions.ConnectionError:
-        print("\n\n‚ùå Erro: N√£o foi poss√≠vel conectar √† API")
-        print("   Certifique-se de que a API est√° rodando:")
-        print("   python api.py")
-    except Exception as e:
-        print(f"\n\n‚ùå Erro inesperado: {e}")
Index: model/model_executor.py
===================================================================
diff --git a/model/model_executor.py b/model/model_executor.py
deleted file mode 100644
--- a/model/model_executor.py	(revision 17f2ab735aa08aa0cd60b304b1c1940161aa971d)
+++ /dev/null	(revision 17f2ab735aa08aa0cd60b304b1c1940161aa971d)
@@ -1,464 +0,0 @@
-import os
-import logging
-from datetime import datetime, timedelta
-import numpy as np
-import pandas as pd
-import torch
-import torch.nn as nn
-import joblib
-import sys
-from pathlib import Path
-
-logger = logging.getLogger(__name__)
-
-sys.path.append(str(Path(__file__).parent / "model"))
-
-
-BASE_DIR = os.path.dirname(os.path.abspath(__file__))
-
-EXPORT_DIR = os.path.join(BASE_DIR, "export")
-DATA_DIR = os.path.join(
-    BASE_DIR,
-    "scrapper",
-    "data",
-    "processed",
-    "prices"
-)
-
-DEVICE = torch.device("cpu")
-
-FEATURE_COLUMNS = ["close", "volume"]
-
-
-class LSTMModel(nn.Module):
-    def __init__(self, input_size, hidden_size, num_layers):
-        super().__init__()
-        self.lstm = nn.LSTM(
-            input_size=input_size,
-            hidden_size=hidden_size,
-            num_layers=num_layers,
-            batch_first=True
-        )
-        self.fc = nn.Linear(hidden_size, 1)
-
-    def forward(self, x):
-        out, _ = self.lstm(x)
-        out = out[:, -1, :]
-        return self.fc(out)
-
-
-_models_cache = {}
-
-
-def load_artifacts(ticker: str):
-    """Carrega modelo e scalers espec√≠ficos para um ticker"""
-    ticker = ticker.upper()
-    logger.info(f"Iniciando carregamento de artefatos para ticker: {ticker}")
-    
-    # Verificar se j√° est√° em cache
-    logger.debug(f"Verificando cache de modelos para {ticker}")
-    if ticker in _models_cache:
-        logger.info(f"Modelo {ticker} encontrado em cache")
-        return _models_cache[ticker]
-    
-    logger.debug(f"Modelo {ticker} n√£o encontrado em cache, carregando do disco")
-    
-    # Caminhos dos arquivos do modelo
-    MODEL_PATH = os.path.join(EXPORT_DIR, f"lstm_model_{ticker}.pth")
-    SCALER_FEATURES_PATH = os.path.join(EXPORT_DIR, f"scaler_features_{ticker}.save")
-    SCALER_CLOSE_PATH = os.path.join(EXPORT_DIR, f"scaler_close_{ticker}.save")
-    
-    logger.debug(f"Verificando exist√™ncia de arquivos do modelo em: {EXPORT_DIR}")
-    
-    # Verificar se os arquivos existem
-    logger.debug(f"Verificando se modelo existe: {MODEL_PATH}")
-    if not os.path.exists(MODEL_PATH):
-        logger.warning(f"Modelo n√£o encontrado para {ticker}. Iniciando treinamento autom√°tico...")
-        
-        try:
-            # Importar fun√ß√£o de treinamento
-            from model.model_training import train_model
-            
-            logger.info(f"Treinando modelo automaticamente para {ticker}")
-            train_result = train_model(ticker=ticker)
-            logger.info(f"Treinamento autom√°tico conclu√≠do para {ticker} com RMSE: {train_result.get('rmse', 'N/A')}")
-            
-            # Verificar novamente ap√≥s treinamento
-            logger.debug(f"Verificando se modelo foi criado ap√≥s treinamento: {MODEL_PATH}")
-            if not os.path.exists(MODEL_PATH):
-                error_msg = f"Falha ao criar modelo para {ticker} ap√≥s treinamento autom√°tico"
-                logger.error(error_msg)
-                raise FileNotFoundError(error_msg)
-            
-        except ImportError as e:
-            error_msg = f"Erro ao importar m√≥dulo de treinamento: {e}"
-            logger.error(error_msg, exc_info=True)
-            raise ImportError(error_msg) from e
-        except Exception as e:
-            error_msg = f"Erro durante treinamento autom√°tico de {ticker}: {e}"
-            logger.error(error_msg, exc_info=True)
-            raise RuntimeError(error_msg) from e
-    
-    logger.debug(f"Verificando se scalers existem: {SCALER_FEATURES_PATH}, {SCALER_CLOSE_PATH}")
-    if not os.path.exists(SCALER_FEATURES_PATH) or not os.path.exists(SCALER_CLOSE_PATH):
-        error_msg = f"Scalers n√£o encontrados para {ticker} em {EXPORT_DIR}"
-        logger.error(error_msg)
-        raise FileNotFoundError(error_msg)
-    
-    # Carregar checkpoint
-    logger.info(f"Carregando checkpoint do modelo de: {MODEL_PATH}")
-    try:
-        checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)
-        logger.debug(f"Checkpoint carregado com sucesso para {ticker}")
-    except Exception as e:
-        error_msg = f"Erro ao carregar checkpoint para {ticker}: {e}"
-        logger.error(error_msg, exc_info=True)
-        raise RuntimeError(error_msg) from e
-    
-    model_config = checkpoint["model_config"]
-    seq_length = model_config["seq_length"]
-    logger.debug(f"Configura√ß√£o do modelo: {model_config}")
-    
-    # Criar e carregar modelo
-    logger.info(f"Criando modelo LSTM para {ticker}")
-    try:
-        model = LSTMModel(
-            input_size=model_config["input_size"],
-            hidden_size=model_config["hidden_size"],
-            num_layers=model_config["num_layers"]
-        )
-        
-        model.load_state_dict(checkpoint["model_state_dict"])
-        model.eval()
-        logger.debug(f"Modelo carregado e definido para modo de avalia√ß√£o")
-    except Exception as e:
-        error_msg = f"Erro ao criar/carregar modelo para {ticker}: {e}"
-        logger.error(error_msg, exc_info=True)
-        raise RuntimeError(error_msg) from e
-    
-    # Carregar scalers
-    logger.info(f"Carregando scalers para {ticker}")
-    try:
-        scaler_features = joblib.load(SCALER_FEATURES_PATH)
-        scaler_close = joblib.load(SCALER_CLOSE_PATH)
-        logger.debug(f"Scalers carregados com sucesso")
-    except Exception as e:
-        error_msg = f"Erro ao carregar scalers para {ticker}: {e}"
-        logger.error(error_msg, exc_info=True)
-        raise RuntimeError(error_msg) from e
-    
-    # Armazenar em cache
-    artifacts = {
-        "model": model,
-        "scaler_features": scaler_features,
-        "scaler_close": scaler_close,
-        "model_config": model_config,
-        "seq_length": seq_length
-    }
-    
-    _models_cache[ticker] = artifacts
-    logger.info(f"Artefatos para {ticker} armazenados em cache com sucesso")
-    
-    return artifacts
-
-# =========================
-# Dados
-# =========================
-
-def load_processed_data(ticker: str) -> pd.DataFrame:
-    logger.info(f"Carregando dados processados para ticker: {ticker}")
-    ticker_dir = f"ticker={ticker}"
-    full_path = os.path.join(DATA_DIR, ticker_dir)
-    logger.debug(f"Caminho dos dados: {full_path}")
-
-    logger.debug(f"Verificando se diret√≥rio existe: {full_path}")
-    if not os.path.exists(full_path):
-        error_msg = f"Dados processados n√£o encontrados para {ticker} em {full_path}"
-        logger.error(error_msg)
-        raise ValueError(error_msg)
-
-    logger.debug(f"Listando arquivos parquet em: {full_path}")
-    files = [f for f in os.listdir(full_path) if f.endswith(".parquet")]
-    logger.debug(f"Arquivos encontrados: {files}")
-    
-    if not files:
-        error_msg = f"Nenhum arquivo parquet encontrado em {full_path}"
-        logger.error(error_msg)
-        raise ValueError(error_msg)
-
-    logger.info(f"Lendo arquivo parquet: {files[0]}")
-    try:
-        df = pd.read_parquet(os.path.join(full_path, files[0]))
-        logger.debug(f"Dados carregados: {len(df)} registros, colunas: {list(df.columns)}")
-    except Exception as e:
-        error_msg = f"Erro ao ler arquivo parquet para {ticker}: {e}"
-        logger.error(error_msg, exc_info=True)
-        raise RuntimeError(error_msg) from e
-
-    required_cols = {"date", "close", "volume"}
-    logger.debug(f"Verificando colunas obrigat√≥rias: {required_cols}")
-    if not required_cols.issubset(df.columns):
-        error_msg = f"Colunas esperadas ausentes para {ticker}. Esperado: {required_cols}, Encontrado: {set(df.columns)}"
-        logger.error(error_msg)
-        raise ValueError(error_msg)
-
-    logger.debug(f"Convertendo coluna 'date' para datetime")
-    df["date"] = pd.to_datetime(df["date"])
-    df = df.sort_values("date").reset_index(drop=True)
-    logger.info(f"Dados processados carregados com sucesso: {len(df)} registros de {df['date'].min()} at√© {df['date'].max()}")
-
-    return df
-
-def filter_date_range(df: pd.DataFrame, start: datetime, end: datetime, seq_length: int) -> pd.DataFrame:
-    logger.info(f"Filtrando dados de {start} at√© {end}")
-    logger.debug(f"Registros totais antes do filtro: {len(df)}")
-    
-    mask = (df["date"] >= start) & (df["date"] <= end)
-    filtered = df.loc[mask]
-    logger.debug(f"Registros ap√≥s filtro: {len(filtered)}")
-
-    logger.debug(f"Verificando se h√° dados suficientes (m√≠nimo {seq_length} registros)")
-    if len(filtered) < seq_length:
-        error_msg = f"Intervalo insuficiente: encontrados {len(filtered)} registros, m√≠nimo necess√°rio {seq_length}"
-        logger.error(error_msg)
-        raise ValueError(error_msg)
-
-    logger.info(f"Dados filtrados com sucesso: {len(filtered)} registros")
-    return filtered
-
-# =========================
-# Prepara√ß√£o do input
-# =========================
-
-def prepare_input_window(df: pd.DataFrame, scaler_features, seq_length: int) -> torch.Tensor:
-    logger.debug(f"Preparando janela de entrada com {seq_length} registros")
-    logger.debug(f"Colunas de features: {FEATURE_COLUMNS}")
-    
-    try:
-        features = df[FEATURE_COLUMNS].values
-        logger.debug(f"Features extra√≠das: shape {features.shape}")
-        
-        scaled = scaler_features.transform(features)
-        logger.debug(f"Features escaladas: shape {scaled.shape}")
-
-        window = scaled[-seq_length:]
-        window = np.expand_dims(window, axis=0)
-        logger.debug(f"Janela preparada: shape {window.shape}")
-
-        tensor = torch.tensor(window, dtype=torch.float32, device=DEVICE)
-        logger.debug(f"Tensor criado: shape {tensor.shape}, device {DEVICE}")
-        return tensor
-    except Exception as e:
-        error_msg = f"Erro ao preparar janela de entrada: {e}"
-        logger.error(error_msg, exc_info=True)
-        raise RuntimeError(error_msg) from e
-
-# =========================
-# Previs√£o para uma data futura
-# =========================
-
-def predict_single_date(
-    ticker: str,
-    start_date: str,
-    end_date: str,
-    horizon: int
-) -> dict:
-    if horizon not in (1, 2, 7):
-        raise ValueError("Horizon permitido: 1, 2 ou 7 dias")
-
-    artifacts = load_artifacts(ticker)
-    model = artifacts["model"]
-    scaler_close = artifacts["scaler_close"]
-    scaler_features = artifacts["scaler_features"]
-    seq_length = artifacts["seq_length"]
-    model_config = artifacts["model_config"]
-
-    df = load_processed_data(ticker)
-
-    start = datetime.fromisoformat(start_date)
-    end = datetime.fromisoformat(end_date)
-
-    df = filter_date_range(df, start, end, seq_length)
-
-    input_tensor = prepare_input_window(df, scaler_features, seq_length)
-
-    with torch.no_grad():
-        pred_scaled = model(input_tensor).cpu().numpy()[0][0]
-
-    pred_price = scaler_close.inverse_transform([[pred_scaled]])[0][0]
-
-    target_date = end + timedelta(days=horizon)
-
-    return {
-        "ticker": ticker,
-        "target_date": target_date.strftime("%Y-%m-%d"),
-        "horizon_days": horizon,
-        "predicted_price": float(round(pred_price, 2)),
-        "model_version": model_config.get("version", "v1")
-    }
-
-# =========================
-# S√©rie de previs√µes (forecast recursivo)
-# =========================
-
-def predict_series(
-    ticker: str,
-    start_date: str,
-    end_date: str,
-    horizon: int
-) -> dict:
-    if horizon not in (1, 2, 7):
-        raise ValueError("Horizon permitido: 1, 2 ou 7 dias")
-
-    artifacts = load_artifacts(ticker)
-    model = artifacts["model"]
-    scaler_close = artifacts["scaler_close"]
-    scaler_features = artifacts["scaler_features"]
-    seq_length = artifacts["seq_length"]
-    model_config = artifacts["model_config"]
-
-    df = load_processed_data(ticker)
-
-    start = datetime.fromisoformat(start_date)
-    end = datetime.fromisoformat(end_date)
-
-    df = filter_date_range(df, start, end, seq_length)
-
-    predictions = []
-    current_df = df.copy()
-
-    for step in range(horizon):
-        input_tensor = prepare_input_window(current_df, scaler_features, seq_length)
-
-        with torch.no_grad():
-            pred_scaled = model(input_tensor).cpu().numpy()[0][0]
-
-        pred_price = scaler_close.inverse_transform([[pred_scaled]])[0][0]
-
-        next_date = current_df["date"].iloc[-1] + timedelta(days=1)
-
-        predictions.append({
-            "date": next_date.strftime("%Y-%m-%d"),
-            "price": float(round(pred_price, 2))
-        })
-
-        new_row = current_df.iloc[-1].copy()
-        new_row["date"] = next_date
-        new_row["close"] = pred_price
-
-        current_df = pd.concat(
-            [current_df, pd.DataFrame([new_row])],
-            ignore_index=True
-        )
-
-    return {
-        "ticker": ticker,
-        "horizon_days": horizon,
-        "predictions": predictions,
-        "model_version": model_config.get("version", "v1")
-    }
-
-
-# =========================
-# Fun√ß√£o de previs√£o simplificada para API
-# =========================
-
-def predict_price(df_processed: pd.DataFrame, ticker: str, days: int = 1) -> dict:
-    """
-    Fun√ß√£o simplificada para previs√£o de pre√ßos usada pela API.
-    
-    Args:
-        df_processed: DataFrame com dados processados do ticker
-        ticker: C√≥digo da a√ß√£o
-        days: N√∫mero de dias para prever (padr√£o: 1)
-    
-    Returns:
-        dict com predictions (lista de pre√ßos), days e last_known_price
-    """
-    ticker = ticker.upper()
-    logger.info(f"Iniciando previs√£o de pre√ßo para {ticker} - {days} dias")
-    
-    # Carregar artefatos do modelo
-    logger.debug(f"Carregando artefatos do modelo para {ticker}")
-    try:
-        artifacts = load_artifacts(ticker)
-        model = artifacts["model"]
-        scaler_close = artifacts["scaler_close"]
-        scaler_features = artifacts["scaler_features"]
-        seq_length = artifacts["seq_length"]
-        logger.debug(f"Artefatos carregados, seq_length: {seq_length}")
-    except Exception as e:
-        error_msg = f"Erro ao carregar artefatos para {ticker}: {e}"
-        logger.error(error_msg, exc_info=True)
-        raise
-    
-    # Preparar dados
-    logger.debug(f"Preparando dados para previs√£o")
-    df = df_processed.copy()
-    df.columns = df.columns.str.lower()
-    
-    logger.debug(f"Verificando se coluna 'date' existe: {'date' in df.columns}")
-    if "date" in df.columns:
-        logger.debug(f"Convertendo e ordenando por data")
-        df["date"] = pd.to_datetime(df["date"])
-        df = df.sort_values("date").reset_index(drop=True)
-    
-    # Verificar se h√° dados suficientes
-    logger.debug(f"Verificando quantidade de dados: {len(df)} registros dispon√≠veis, {seq_length} necess√°rios")
-    if len(df) < seq_length:
-        error_msg = f"Dados insuficientes para {ticker}. M√≠nimo necess√°rio: {seq_length} registros. Dispon√≠vel: {len(df)}"
-        logger.error(error_msg)
-        raise ValueError(error_msg)
-    
-    # Obter √∫ltimo pre√ßo conhecido
-    last_known_price = float(df["close"].iloc[-1])
-    logger.info(f"√öltimo pre√ßo conhecido de {ticker}: R$ {last_known_price:.2f}")
-    
-    # Fazer previs√µes recursivas
-    predictions = []
-    current_df = df.copy()
-    logger.info(f"Iniciando previs√µes recursivas para {days} dias")
-    
-    for step in range(days):
-        logger.debug(f"Previs√£o do dia {step + 1}/{days}")
-        
-        try:
-            # Preparar janela de entrada
-            logger.debug(f"Preparando janela de entrada para dia {step + 1}")
-            features = current_df[FEATURE_COLUMNS].values
-            scaled = scaler_features.transform(features)
-            window = scaled[-seq_length:]
-            window = np.expand_dims(window, axis=0)
-            input_tensor = torch.tensor(window, dtype=torch.float32, device=DEVICE)
-            
-            # Fazer previs√£o
-            logger.debug(f"Executando modelo de previs√£o")
-            with torch.no_grad():
-                pred_scaled = model(input_tensor).cpu().numpy()[0][0]
-            
-            pred_price = scaler_close.inverse_transform([[pred_scaled]])[0][0]
-            predictions.append(float(pred_price))
-            logger.debug(f"Previs√£o dia {step + 1}: R$ {pred_price:.2f}")
-            
-            # Adicionar previs√£o ao dataframe para pr√≥xima itera√ß√£o
-            if step < days - 1:  # N√£o precisa adicionar na √∫ltima itera√ß√£o
-                logger.debug(f"Adicionando previs√£o ao dataset para pr√≥xima itera√ß√£o")
-                new_row = current_df.iloc[-1].copy()
-                new_row["close"] = pred_price
-                
-                # Manter o volume (pode ser melhorado com previs√£o de volume tamb√©m)
-                current_df = pd.concat(
-                    [current_df, pd.DataFrame([new_row])],
-                    ignore_index=True
-                )
-        except Exception as e:
-            error_msg = f"Erro ao fazer previs√£o do dia {step + 1} para {ticker}: {e}"
-            logger.error(error_msg, exc_info=True)
-            raise RuntimeError(error_msg) from e
-    
-    logger.info(f"Previs√µes conclu√≠das para {ticker}: {[f'R$ {p:.2f}' for p in predictions]}")
-    return {
-        "predictions": predictions,
-        "days": days,
-        "last_known_price": last_known_price
-    }
Index: Dockerfile
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>FROM python:3.13-slim\n\nWORKDIR /app\n\nRUN apt-get update && apt-get install -y \\\n    gcc \\\n    && rm -rf /var/lib/apt/lists/*\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY model/ ./model/\nCOPY api.py .\nCOPY export/ ./export/\nCOPY scrapper/ ./scrapper/\n\nENV PYTHONUNBUFFERED=1\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Dockerfile b/Dockerfile
--- a/Dockerfile	(revision 17f2ab735aa08aa0cd60b304b1c1940161aa971d)
+++ b/Dockerfile	(date 1767837023130)
@@ -1,4 +1,4 @@
-FROM python:3.13-slim
+FROM python:3.11-slim
 
 WORKDIR /app
 
@@ -9,10 +9,10 @@
 COPY requirements.txt .
 RUN pip install --no-cache-dir -r requirements.txt
 
-COPY model/ ./model/
+COPY model_executor.py .
 COPY api.py .
 COPY export/ ./export/
-COPY scrapper/ ./scrapper/
+COPY scapper/ ./scapper/
 
 ENV PYTHONUNBUFFERED=1
 
Index: requirements.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># API Framework\nfastapi==0.115.6\nuvicorn[standard]==0.32.1\npydantic==2.10.6\n\n# Machine Learning\ntorch==2.5.1\nscikit-learn==1.5.2\njoblib==1.4.2\nnumpy==1.26.4\n\n# MLFlow para tracking de experimentos (SEMPRE HABILITADO)\nmlflow==2.16.2\n\n# Data Processing\npandas==2.3.3\nscipy==1.14.0\npyarrow==15.0.0\n\n# Data Extraction\nyfinance==0.2.40\nbeautifulsoup4==4.12.3\nrequests==2.32.5\n\n# AWS S3 (opcional - para STORAGE_TYPE=s3)\nboto3==1.35.94\nbotocore==1.35.94\n\n# Utilities\npython-dateutil==2.9.0.post0\npytz\nurllib3==2.2.1\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/requirements.txt b/requirements.txt
--- a/requirements.txt	(revision 17f2ab735aa08aa0cd60b304b1c1940161aa971d)
+++ b/requirements.txt	(date 1767837023323)
@@ -4,17 +4,17 @@
 pydantic==2.10.6
 
 # Machine Learning
-torch==2.5.1
-scikit-learn==1.5.2
-joblib==1.4.2
+torch==2.2.0
+scikit-learn==1.4.0
+joblib==1.3.2
 numpy==1.26.4
 
-# MLFlow para tracking de experimentos (SEMPRE HABILITADO)
-mlflow==2.16.2
+# MLFlow para tracking de experimentos
+mlflow==2.10.2
 
 # Data Processing
-pandas==2.3.3
-scipy==1.14.0
+pandas==2.2.0
+scipy==1.12.0
 pyarrow==15.0.0
 
 # Data Extraction
@@ -28,5 +28,5 @@
 
 # Utilities
 python-dateutil==2.9.0.post0
-pytz
+pytz==2024.1
 urllib3==2.2.1
Index: docker-compose.yml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>version: '3.8'\n\nservices:\n  api:\n    build: .\n    container_name: stock-prediction-api\n    ports:\n      - \"8000:8000\"\n    environment:\n      - PYTHONUNBUFFERED=1\n    volumes:\n      - ./scrapper/data:/app/scrapper/data\n      - ./export:/app/export\n      - ./mlruns:/app/mlruns\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 40s\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/docker-compose.yml b/docker-compose.yml
--- a/docker-compose.yml	(revision 17f2ab735aa08aa0cd60b304b1c1940161aa971d)
+++ b/docker-compose.yml	(date 1767837023323)
@@ -9,9 +9,8 @@
     environment:
       - PYTHONUNBUFFERED=1
     volumes:
-      - ./scrapper/data:/app/scrapper/data
+      - ./scapper/data:/app/scapper/data
       - ./export:/app/export
-      - ./mlruns:/app/mlruns
     restart: unless-stopped
     healthcheck:
       test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
Index: model/model_training.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import yfinance as yf\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport joblib\nimport os\nimport sys\nfrom pathlib import Path\nfrom datetime import datetime\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport mlflow\nimport mlflow.pytorch\n\nMLFLOW_ENABLED = True\n\nsys.path.append(str(Path(__file__).parent.parent / \"scrapper\"))\n\nfrom scrapper_pipeline import get_or_scrappe_ticker\n\n\nSEQ_LENGTH = 30\nTRAIN_SPLIT = 0.8\nEPOCHS = 50\nLEARNING_RATE = 0.001\n\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nEXPORT_DIR = os.path.join(BASE_DIR, \"export\")\nos.makedirs(EXPORT_DIR, exist_ok=True)\n\nMLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"file:./mlruns\")\nMLFLOW_EXPERIMENT_NAME = os.getenv(\"MLFLOW_EXPERIMENT_NAME\", \"stock-price-prediction\")\n\nif MLFLOW_ENABLED:\n    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n    mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\n\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size=2, hidden_size=64, num_layers=2):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        out = out[:, -1, :]\n        return self.fc(out)\n\n\ndef create_sequences(X, y, seq_length):\n    Xs, ys = [], []\n    for i in range(len(X) - seq_length):\n        Xs.append(X[i:i + seq_length])\n        ys.append(y[i + seq_length])\n    return np.array(Xs), np.array(ys)\n\n\ndef train_model(ticker: str, start_date: str = \"2020-01-01\", end_date: str = None) -> dict:\n    \"\"\"\n    Treina um modelo LSTM para um ticker espec√≠fico.\n    \n    Args:\n        ticker: C√≥digo da a√ß√£o (ex: PETR4.SA, VALE3.SA)\n        start_date: Data inicial para buscar dados\n        end_date: Data final para buscar dados (opcional, padr√£o: hoje)\n    \n    Returns:\n        dict com informa√ß√µes do treinamento e m√©tricas\n    \"\"\"\n    if end_date is None:\n        end_date = datetime.now().strftime(\"%Y-%m-%d\")\n    \n    ticker = ticker.upper()\n    \n    if MLFLOW_ENABLED:\n        mlflow.start_run(run_name=f\"train_{ticker}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n        \n        mlflow.log_param(\"ticker\", ticker)\n        mlflow.log_param(\"start_date\", start_date)\n        mlflow.log_param(\"end_date\", end_date)\n        mlflow.log_param(\"seq_length\", SEQ_LENGTH)\n        mlflow.log_param(\"train_split\", TRAIN_SPLIT)\n        mlflow.log_param(\"epochs\", EPOCHS)\n        mlflow.log_param(\"learning_rate\", LEARNING_RATE)\n        mlflow.log_param(\"hidden_size\", 64)\n        mlflow.log_param(\"num_layers\", 2)\n    \n    try:\n        print(f\"\\n{'='*50}\")\n        print(f\"Iniciando treinamento para {ticker}\")\n        print(f\"{'='*50}\\n\")\n        \n        \n        print(\"Obtendo dados hist√≥ricos...\")\n        df = get_or_scrappe_ticker(\n            ticker=ticker,\n            data_path=\"scrapper/data/processed/prices\",\n            start_date=start_date,\n            end_date=end_date\n        )\n        \n        if df is None or df.empty:\n            raise ValueError(f\"Nenhum dado retornado para o ticker {ticker}\")\n        \n        df = df[df[\"ticker\"] == ticker].copy()\n        df = df[[\"close\", \"volume\"]].dropna()\n        \n        print(f\"Dados obtidos: {len(df)} registros de {df.index.min()} at√© {df.index.max()}\")\n        \n        if MLFLOW_ENABLED:\n            mlflow.log_metric(\"dataset_size\", len(df))\n        \n        \n        X = df[[\"close\", \"volume\"]].values\n        y = df[[\"close\"]].values\n        \n        \n        feature_scaler = MinMaxScaler()\n        close_scaler = MinMaxScaler()\n        \n        X_scaled = feature_scaler.fit_transform(X)\n        y_scaled = close_scaler.fit_transform(y)\n        \n        \n        train_size = int(len(X_scaled) * TRAIN_SPLIT)\n        \n        X_train_raw = X_scaled[:train_size]\n        y_train_raw = y_scaled[:train_size]\n        \n        X_test_raw = X_scaled[train_size - SEQ_LENGTH:]\n        y_test_raw = y_scaled[train_size - SEQ_LENGTH:]\n        \n        X_train, y_train = create_sequences(X_train_raw, y_train_raw, SEQ_LENGTH)\n        X_test, y_test = create_sequences(X_test_raw, y_test_raw, SEQ_LENGTH)\n        \n        print(f\"Dados de treino: {len(X_train)} sequ√™ncias\")\n        print(f\"Dados de teste: {len(X_test)} sequ√™ncias\")\n        \n        if MLFLOW_ENABLED:\n            mlflow.log_metric(\"train_sequences\", len(X_train))\n            mlflow.log_metric(\"test_sequences\", len(X_test))\n        \n        \n        X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n        y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n        \n        X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n        y_test_torch = torch.tensor(y_test, dtype=torch.float32)\n        \n        \n        model = LSTMModel(\n            input_size=2,\n            hidden_size=64,\n            num_layers=2\n        )\n        \n        criterion = nn.MSELoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n        \n        \n        print(f\"\\nIniciando treinamento ({EPOCHS} √©pocas)...\")\n        \n        for epoch in range(EPOCHS):\n            model.train()\n            optimizer.zero_grad()\n            \n            output = model(X_train_torch)\n            loss = criterion(output, y_train_torch)\n            \n            loss.backward()\n            optimizer.step()\n            \n            if MLFLOW_ENABLED:\n                mlflow.log_metric(\"train_loss\", loss.item(), step=epoch)\n            \n            if (epoch + 1) % 10 == 0:\n                print(f\"√âpoca {epoch + 1}/{EPOCHS} - Loss: {loss.item():.6f}\")\n        \n        \n        print(\"\\nExecutando backtest...\")\n        model.eval()\n        with torch.no_grad():\n            preds_scaled = model(X_test_torch).numpy()\n        \n        preds = close_scaler.inverse_transform(preds_scaled)\n        reals = close_scaler.inverse_transform(y_test_torch.numpy())\n        \n        rmse = np.sqrt(mean_squared_error(reals, preds))\n        mae = np.mean(np.abs(reals - preds))\n        mape = np.mean(np.abs((reals - preds) / reals)) * 100\n        \n        print(\"\\n===== RESULTADOS DO BACKTEST =====\")\n        print(f\"Ticker: {ticker}\")\n        print(f\"RMSE: R$ {rmse:.2f}\")\n        print(f\"MAE: R$ {mae:.2f}\")\n        print(f\"MAPE: {mape:.2f}%\")\n        print(f\"Pre√ßo real √∫ltimo dia: R$ {reals[-1][0]:.2f}\")\n        print(f\"Pre√ßo previsto √∫ltimo dia: R$ {preds[-1][0]:.2f}\")\n        print(f\"Erro absoluto: R$ {abs(reals[-1][0] - preds[-1][0]):.2f}\")\n        \n        if MLFLOW_ENABLED:\n            mlflow.log_metric(\"rmse\", rmse)\n            mlflow.log_metric(\"mae\", mae)\n            mlflow.log_metric(\"mape\", mape)\n            mlflow.log_metric(\"last_real_price\", float(reals[-1][0]))\n            mlflow.log_metric(\"last_predicted_price\", float(preds[-1][0]))\n            mlflow.log_metric(\"last_absolute_error\", float(abs(reals[-1][0] - preds[-1][0])))\n        \n        \n        last_sequence = X_scaled[-SEQ_LENGTH:]\n        last_sequence = torch.tensor(last_sequence, dtype=torch.float32).unsqueeze(0)\n        \n        with torch.no_grad():\n            next_scaled = model(last_sequence).item()\n        \n        next_price = close_scaler.inverse_transform([[next_scaled]])[0][0]\n        print(f\"\\nPre√ßo previsto pr√≥ximo dia ({ticker}): R$ {next_price:.2f}\")\n        \n        if MLFLOW_ENABLED:\n            mlflow.log_metric(\"next_day_prediction\", next_price)\n        \n        \n        MODEL_PATH = os.path.join(EXPORT_DIR, f\"lstm_model_{ticker}.pth\")\n        SCALER_FEATURES_PATH = os.path.join(EXPORT_DIR, f\"scaler_features_{ticker}.save\")\n        SCALER_CLOSE_PATH = os.path.join(EXPORT_DIR, f\"scaler_close_{ticker}.save\")\n        \n        checkpoint = {\n            \"model_state_dict\": model.state_dict(),\n            \"model_config\": {\n                \"input_size\": 2,\n                \"hidden_size\": 64,\n                \"num_layers\": 2,\n                \"seq_length\": SEQ_LENGTH\n            },\n            \"version\": \"v1\",\n            \"trained_on\": ticker,\n            \"trained_at\": datetime.utcnow().isoformat(),\n            \"metrics\": {\n                \"rmse\": float(rmse),\n                \"mae\": float(mae),\n                \"mape\": float(mape),\n                \"last_real_price\": float(reals[-1][0]),\n                \"last_predicted_price\": float(preds[-1][0]),\n                \"next_prediction\": float(next_price)\n            }\n        }\n        \n        torch.save(checkpoint, MODEL_PATH)\n        joblib.dump(feature_scaler, SCALER_FEATURES_PATH)\n        joblib.dump(close_scaler, SCALER_CLOSE_PATH)\n        \n        print(f\"\\n‚úì Modelo salvo em: {MODEL_PATH}\")\n        print(f\"‚úì Scaler features salvo em: {SCALER_FEATURES_PATH}\")\n        print(f\"‚úì Scaler close salvo em: {SCALER_CLOSE_PATH}\")\n        \n        if MLFLOW_ENABLED:\n            mlflow.pytorch.log_model(model, \"model\")\n            \n            mlflow.log_artifact(MODEL_PATH, \"checkpoints\")\n            mlflow.log_artifact(SCALER_FEATURES_PATH, \"scalers\")\n            mlflow.log_artifact(SCALER_CLOSE_PATH, \"scalers\")\n            \n            mlflow.set_tag(\"ticker\", ticker)\n            mlflow.set_tag(\"model_type\", \"LSTM\")\n            mlflow.set_tag(\"framework\", \"PyTorch\")\n        \n        print(f\"\\n{'='*50}\")\n        print(\"Treinamento conclu√≠do com sucesso!\")\n        print(f\"{'='*50}\\n\")\n        \n        result = {\n            \"ticker\": ticker,\n            \"rmse\": float(rmse),\n            \"mae\": float(mae),\n            \"mape\": float(mape),\n            \"last_real_price\": float(reals[-1][0]),\n            \"last_predicted_price\": float(preds[-1][0]),\n            \"next_prediction\": float(next_price),\n            \"model_path\": MODEL_PATH,\n            \"trained_at\": checkpoint[\"trained_at\"]\n        }\n        \n        if MLFLOW_ENABLED:\n            mlflow.end_run()\n        \n        return result\n        \n    except Exception as e:\n        if MLFLOW_ENABLED:\n            mlflow.end_run(status=\"FAILED\")\n        raise e\n\n\n\nif __name__ == \"__main__\":\n    ticker = input(\"Digite o c√≥digo da a√ß√£o (ex: PETR4.SA, VALE3.SA, ITUB4.SA): \").strip()\n    \n    if not ticker:\n        print(\"Ticker n√£o pode ser vazio!\")\n        exit(1)\n    \n    try:\n        result = train_model(ticker)\n        print(\"\\nResumo do treinamento:\")\n        print(f\"  - RMSE: R$ {result['rmse']:.2f}\")\n        print(f\"  - Pr√≥xima previs√£o: R$ {result['next_prediction']:.2f}\")\n    except Exception as e:\n        print(f\"\\nErro durante o treinamento: {e}\")\n        exit(1)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/model/model_training.py b/model/model_training.py
--- a/model/model_training.py	(revision 17f2ab735aa08aa0cd60b304b1c1940161aa971d)
+++ b/model/model_training.py	(date 1767837023323)
@@ -10,10 +10,13 @@
 from sklearn.preprocessing import MinMaxScaler
 from sklearn.metrics import mean_squared_error
 
-import mlflow
-import mlflow.pytorch
-
-MLFLOW_ENABLED = True
+try:
+    import mlflow
+    import mlflow.pytorch
+    MLFLOW_ENABLED = True
+except ImportError:
+    MLFLOW_ENABLED = False
+    print("MLFlow n√£o instalado. Instale com: pip install mlflow")
 
 sys.path.append(str(Path(__file__).parent.parent / "scrapper"))
 
Index: api.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from fastapi import FastAPI, HTTPException, BackgroundTasks\nfrom pydantic import BaseModel\nfrom typing import Optional\nfrom datetime import datetime, timedelta\nimport logging\nimport sys\nfrom pathlib import Path\n\nsys.path.append(str(Path(__file__).parent / \"scrapper\"))\nsys.path.append(str(Path(__file__).parent / \"model\"))\n\nfrom model.model_executor import predict_price\nfrom scrapper_pipeline import get_or_scrappe_ticker\nfrom model_training import train_model\n\napp = FastAPI(\n    title=\"Stock Price Prediction API\",\n    description=\"API para previs√£o de pre√ßos de a√ß√µes usando LSTM\",\n    version=\"1.0.0\"\n)\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n)\nlogger = logging.getLogger(__name__)\n\n# Configurar MLFlow\nMLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"file:./mlruns\")\nMLFLOW_EXPERIMENT_NAME = os.getenv(\"MLFLOW_EXPERIMENT_NAME\", \"stock-price-prediction\")\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\nmlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)\nlogger.info(f\"MLFlow configurado - URI: {MLFLOW_TRACKING_URI}, Experimento: {MLFLOW_EXPERIMENT_NAME}\")\n\n\nclass PredictionRequest(BaseModel):\n    ticker: str\n    start_date: Optional[str] = None\n    end_date: Optional[str] = None\n    days: Optional[int] = 1\n\n\nclass TrainRequest(BaseModel):\n    ticker: str\n    start_date: Optional[str] = \"2020-01-01\"\n    end_date: Optional[str] = None\n\n\nclass PredictionResponse(BaseModel):\n    ticker: str\n    predictions: list[float]\n    days: int\n    last_known_price: float\n    currency: str = \"BRL\"\n\n\nclass TrainResponse(BaseModel):\n    ticker: str\n    status: str\n    message: str\n    rmse: Optional[float] = None\n    next_prediction: Optional[float] = None\n    trained_at: Optional[str] = None\n\n\nclass HistoricalDataPoint(BaseModel):\n    date: str\n    close: float\n    volume: float\n\n\nclass CustomPredictionRequest(BaseModel):\n    historical_data: list[HistoricalDataPoint]\n    days: int = 1\n    ticker_name: Optional[str] = \"CUSTOM\"\n\n\nclass CustomPredictionResponse(BaseModel):\n    ticker_name: str\n    predictions: list[float]\n    days: int\n    last_known_price: float\n    rmse: float\n    training_samples: int\n    message: str\n\n\n@app.get(\"/\")\ndef root():\n    return {\n        \"message\": \"Stock Price Prediction API\",\n        \"endpoints\": {\n            \"predict\": \"/predict/{ticker}\",\n            \"predict_post\": \"/predict\",\n            \"predict_custom\": \"/predict-custom\",\n            \"train\": \"/train\",\n            \"health\": \"/health\"\n        }\n    }\n\n\n@app.get(\"/health\")\ndef health_check():\n    return {\"status\": \"healthy\"}\n\n\n@app.get(\"/predict/{ticker}\", response_model=PredictionResponse)\ndef predict_ticker_get(ticker: str, days: int = 1, start_date: Optional[str] = None, end_date: Optional[str] = None):\n    try:\n        logger.info(f\"Recebida requisi√ß√£o de previs√£o para ticker: {ticker}, dias: {days}\")\n        \n        df_processed = get_or_scrappe_ticker(\n            ticker=ticker.upper(),\n            data_path=\"scapper/data/processed/prices\",\n            start_date=start_date,\n            end_date=end_date\n        )\n        \n        if df_processed is None or df_processed.empty:\n            raise HTTPException(\n                status_code=404,\n                detail=f\"N√£o foi poss√≠vel obter dados para o ticker {ticker}\"\n            )\n        \n        result = predict_price(df_processed, ticker=ticker.upper(), days=days)\n        \n        logger.info(f\"Previs√£o conclu√≠da para {ticker}: {days} dias\")\n        \n        return PredictionResponse(\n            ticker=ticker.upper(),\n            predictions=[round(p, 2) for p in result['predictions']],\n            days=result['days'],\n            last_known_price=round(result['last_known_price'], 2)\n        )\n        \n    except ValueError as e:\n        logger.error(f\"Erro de valida√ß√£o: {e}\")\n        raise HTTPException(status_code=400, detail=str(e))\n    except Exception as e:\n        logger.error(f\"Erro ao processar previs√£o: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Erro interno: {str(e)}\")\n\n\n@app.post(\"/predict\", response_model=PredictionResponse)\ndef predict_ticker_post(request: PredictionRequest):\n    try:\n        logger.info(f\"Recebida requisi√ß√£o POST de previs√£o para ticker: {request.ticker}, dias: {request.days}\")\n        \n        # Iniciar tracking MLFlow\n        with mlflow.start_run(run_name=f\"predict_POST_{request.ticker.upper()}_{request.days}days\"):\n            mlflow.log_param(\"ticker\", request.ticker.upper())\n            mlflow.log_param(\"days\", request.days)\n            mlflow.log_param(\"endpoint\", \"POST /predict\")\n            if request.start_date:\n                mlflow.log_param(\"start_date\", request.start_date)\n            if request.end_date:\n                mlflow.log_param(\"end_date\", request.end_date)\n            \n            df_processed = get_or_scrappe_ticker(\n                ticker=request.ticker.upper(),\n                data_path=\"scapper/data/processed/prices\",\n                start_date=request.start_date,\n                end_date=request.end_date\n            )\n            \n            if df_processed is None or df_processed.empty:\n                mlflow.log_param(\"status\", \"error_no_data\")\n                raise HTTPException(\n                    status_code=404,\n                    detail=f\"N√£o foi poss√≠vel obter dados para o ticker {request.ticker}\"\n                )\n            \n            mlflow.log_metric(\"data_points\", len(df_processed))\n            \n            result = predict_price(df_processed, ticker=request.ticker.upper(), days=request.days)\n            \n            # Log das previs√µes\n            mlflow.log_metric(\"last_known_price\", result['last_known_price'])\n            for i, pred in enumerate(result['predictions'], 1):\n                mlflow.log_metric(f\"prediction_day_{i}\", pred)\n            mlflow.log_param(\"status\", \"success\")\n            \n            logger.info(f\"Previs√£o conclu√≠da para {request.ticker}: {request.days} dias\")\n            \n            return PredictionResponse(\n                ticker=request.ticker.upper(),\n                predictions=[round(p, 2) for p in result['predictions']],\n                days=result['days'],\n                last_known_price=round(result['last_known_price'], 2)\n            )\n        \n    except ValueError as e:\n        logger.error(f\"Erro de valida√ß√£o: {e}\")\n        raise HTTPException(status_code=400, detail=str(e))\n    except Exception as e:\n        logger.error(f\"Erro ao processar previs√£o: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Erro interno: {str(e)}\")\n\n\n@app.post(\"/train\", response_model=TrainResponse)\ndef train_ticker(request: TrainRequest):\n    \"\"\"\n    Treina um modelo LSTM para um ticker espec√≠fico.\n    \n    O treinamento pode levar alguns minutos dependendo da quantidade de dados.\n    \"\"\"\n    try:\n        logger.info(f\"Iniciando treinamento para ticker: {request.ticker}\")\n        \n        result = train_model(\n            ticker=request.ticker.upper(),\n            start_date=request.start_date,\n            end_date=request.end_date\n        )\n        \n        logger.info(f\"Treinamento conclu√≠do para {request.ticker}\")\n        \n        return TrainResponse(\n            ticker=request.ticker.upper(),\n            status=\"success\",\n            message=f\"Modelo treinado com sucesso para {request.ticker.upper()}\",\n            rmse=result.get(\"rmse\"),\n            next_prediction=result.get(\"next_prediction\"),\n            trained_at=result.get(\"trained_at\")\n        )\n        \n    except ValueError as e:\n        logger.error(f\"Erro de valida√ß√£o no treinamento: {e}\")\n        raise HTTPException(status_code=400, detail=str(e))\n    except Exception as e:\n        logger.error(f\"Erro ao treinar modelo: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Erro no treinamento: {str(e)}\")\n\n\n@app.post(\"/predict-custom\", response_model=CustomPredictionResponse)\ndef predict_custom_data(request: CustomPredictionRequest):\n    \"\"\"\n    Endpoint isolado que recebe dados hist√≥ricos personalizados,\n    treina um modelo tempor√°rio e retorna previs√µes.\n    \n    Este endpoint n√£o salva o modelo e funciona de forma completamente independente.\n    O modelo √© treinado em mem√≥ria apenas para esta requisi√ß√£o.\n    \n    Args:\n        historical_data: Lista de pontos hist√≥ricos com date, close e volume\n        days: N√∫mero de dias para prever (padr√£o: 1)\n        ticker_name: Nome opcional para identifica√ß√£o (padr√£o: \"CUSTOM\")\n    \n    Returns:\n        Previs√µes, m√©tricas de treinamento e informa√ß√µes do processo\n    \"\"\"\n    try:\n        import pandas as pd\n        import numpy as np\n        import torch\n        import torch.nn as nn\n        from sklearn.preprocessing import MinMaxScaler\n        from sklearn.metrics import mean_squared_error\n        \n        logger.info(f\"Recebida requisi√ß√£o de predi√ß√£o customizada: {len(request.historical_data)} pontos, {request.days} dias\")\n        \n        # Iniciar tracking MLFlow\n        with mlflow.start_run(run_name=f\"predict_custom_{request.ticker_name}_{request.days}days\"):\n            mlflow.log_param(\"ticker_name\", request.ticker_name)\n            mlflow.log_param(\"days\", request.days)\n            mlflow.log_param(\"endpoint\", \"POST /predict-custom\")\n            mlflow.log_metric(\"historical_data_points\", len(request.historical_data))\n        \n            if len(request.historical_data) < 30:\n                raise HTTPException(\n                    status_code=400,\n                    detail=f\"M√≠nimo de 30 pontos hist√≥ricos necess√°rios. Fornecidos: {len(request.historical_data)}\"\n                )\n        \n        data_dict = {\n            'date': [pd.to_datetime(point.date) for point in request.historical_data],\n            'close': [point.close for point in request.historical_data],\n            'volume': [point.volume for point in request.historical_data]\n        }\n        df = pd.DataFrame(data_dict)\n        df = df.sort_values('date').reset_index(drop=True)\n        \n        SEQ_LENGTH = 30\n        TRAIN_SPLIT = 0.8\n        EPOCHS = 50\n        LEARNING_RATE = 0.001\n        \n        X = df[['close', 'volume']].values\n        y = df[['close']].values\n        \n        feature_scaler = MinMaxScaler()\n        close_scaler = MinMaxScaler()\n        \n        X_scaled = feature_scaler.fit_transform(X)\n        y_scaled = close_scaler.fit_transform(y)\n        \n        def create_sequences(X, y, seq_length):\n            Xs, ys = [], []\n            for i in range(len(X) - seq_length):\n                Xs.append(X[i:i + seq_length])\n                ys.append(y[i + seq_length])\n            return np.array(Xs), np.array(ys)\n        \n        train_size = int(len(X_scaled) * TRAIN_SPLIT)\n        \n        X_train_raw = X_scaled[:train_size]\n        y_train_raw = y_scaled[:train_size]\n        \n        X_test_raw = X_scaled[train_size - SEQ_LENGTH:]\n        y_test_raw = y_scaled[train_size - SEQ_LENGTH:]\n        \n        X_train, y_train = create_sequences(X_train_raw, y_train_raw, SEQ_LENGTH)\n        X_test, y_test = create_sequences(X_test_raw, y_test_raw, SEQ_LENGTH)\n        \n        X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n        y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n        \n        X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n        y_test_torch = torch.tensor(y_test, dtype=torch.float32)\n        \n        class LSTMModel(nn.Module):\n            def __init__(self, input_size=2, hidden_size=64, num_layers=2):\n                super().__init__()\n                self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n                self.fc = nn.Linear(hidden_size, 1)\n            \n            def forward(self, x):\n                out, _ = self.lstm(x)\n                out = out[:, -1, :]\n                return self.fc(out)\n        \n        model = LSTMModel(input_size=2, hidden_size=64, num_layers=2)\n        criterion = nn.MSELoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n        \n        logger.info(f\"Iniciando treinamento tempor√°rio com {len(X_train)} sequ√™ncias\")\n        model.train()\n        for epoch in range(EPOCHS):\n            optimizer.zero_grad()\n            output = model(X_train_torch)\n            loss = criterion(output, y_train_torch)\n            loss.backward()\n            optimizer.step()\n            \n            if (epoch + 1) % 10 == 0:\n                logger.info(f\"√âpoca {epoch + 1}/{EPOCHS} - Loss: {loss.item():.6f}\")\n        \n        model.eval()\n        with torch.no_grad():\n            preds_scaled = model(X_test_torch).numpy()\n        \n        preds = close_scaler.inverse_transform(preds_scaled)\n        reals = close_scaler.inverse_transform(y_test_torch.numpy())\n        \n        rmse = np.sqrt(mean_squared_error(reals, preds))\n        \n        # Log m√©tricas de treino no MLFlow\n        mlflow.log_metric(\"rmse\", rmse)\n        mlflow.log_metric(\"train_samples\", len(X_train))\n        mlflow.log_metric(\"test_samples\", len(X_test))\n        mlflow.log_param(\"seq_length\", SEQ_LENGTH)\n        mlflow.log_param(\"epochs\", EPOCHS)\n        mlflow.log_param(\"learning_rate\", LEARNING_RATE)\n        \n        logger.info(f\"RMSE do modelo tempor√°rio: {rmse:.2f}\")\n        \n        predictions = []\n        current_data = X_scaled.copy()\n        \n        for step in range(request.days):\n            window = current_data[-SEQ_LENGTH:]\n            window = window.reshape(1, SEQ_LENGTH, 2)\n            input_tensor = torch.tensor(window, dtype=torch.float32)\n            \n            with torch.no_grad():\n                pred_scaled_value = model(input_tensor).cpu().numpy()[0][0]\n            \n            pred_price = close_scaler.inverse_transform([[pred_scaled_value]])[0][0]\n            predictions.append(float(pred_price))\n            \n            if step < request.days - 1:\n                last_volume_scaled = current_data[-1, 1]\n                new_point = np.array([[pred_scaled_value, last_volume_scaled]])\n                current_data = np.vstack([current_data, new_point])\n        \n        last_known_price = float(df['close'].iloc[-1])\n        \n        # Log das previs√µes no MLFlow\n        mlflow.log_metric(\"last_known_price\", last_known_price)\n        for i, pred in enumerate(predictions, 1):\n            mlflow.log_metric(f\"prediction_day_{i}\", pred)\n        mlflow.log_param(\"status\", \"success\")\n        \n        logger.info(f\"Previs√£o customizada conclu√≠da: {request.days} dias\")\n        \n        return CustomPredictionResponse(\n            ticker_name=request.ticker_name,\n            predictions=[round(p, 2) for p in predictions],\n            days=request.days,\n            last_known_price=round(last_known_price, 2),\n            rmse=round(rmse, 2),\n            training_samples=len(X_train),\n            message=f\"Modelo treinado e previs√£o realizada com sucesso usando {len(request.historical_data)} pontos hist√≥ricos\"\n        )\n        \n    except HTTPException:\n        raise\n    except ValueError as e:\n        logger.error(f\"Erro de valida√ß√£o: {e}\")\n        raise HTTPException(status_code=400, detail=str(e))\n    except Exception as e:\n        logger.error(f\"Erro ao processar predi√ß√£o customizada: {e}\")\n        import traceback\n        traceback.print_exc()\n        raise HTTPException(status_code=500, detail=f\"Erro interno: {str(e)}\")\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/api.py b/api.py
--- a/api.py	(revision 17f2ab735aa08aa0cd60b304b1c1940161aa971d)
+++ b/api.py	(date 1767837023323)
@@ -9,7 +9,7 @@
 sys.path.append(str(Path(__file__).parent / "scrapper"))
 sys.path.append(str(Path(__file__).parent / "model"))
 
-from model.model_executor import predict_price
+from model_executor import predict_price
 from scrapper_pipeline import get_or_scrappe_ticker
 from model_training import train_model
 
@@ -25,13 +25,6 @@
 )
 logger = logging.getLogger(__name__)
 
-# Configurar MLFlow
-MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "file:./mlruns")
-MLFLOW_EXPERIMENT_NAME = os.getenv("MLFLOW_EXPERIMENT_NAME", "stock-price-prediction")
-mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
-mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)
-logger.info(f"MLFlow configurado - URI: {MLFLOW_TRACKING_URI}, Experimento: {MLFLOW_EXPERIMENT_NAME}")
-
 
 class PredictionRequest(BaseModel):
     ticker: str
@@ -146,48 +139,29 @@
     try:
         logger.info(f"Recebida requisi√ß√£o POST de previs√£o para ticker: {request.ticker}, dias: {request.days}")
         
-        # Iniciar tracking MLFlow
-        with mlflow.start_run(run_name=f"predict_POST_{request.ticker.upper()}_{request.days}days"):
-            mlflow.log_param("ticker", request.ticker.upper())
-            mlflow.log_param("days", request.days)
-            mlflow.log_param("endpoint", "POST /predict")
-            if request.start_date:
-                mlflow.log_param("start_date", request.start_date)
-            if request.end_date:
-                mlflow.log_param("end_date", request.end_date)
-            
-            df_processed = get_or_scrappe_ticker(
-                ticker=request.ticker.upper(),
-                data_path="scapper/data/processed/prices",
-                start_date=request.start_date,
-                end_date=request.end_date
-            )
-            
-            if df_processed is None or df_processed.empty:
-                mlflow.log_param("status", "error_no_data")
-                raise HTTPException(
-                    status_code=404,
-                    detail=f"N√£o foi poss√≠vel obter dados para o ticker {request.ticker}"
-                )
-            
-            mlflow.log_metric("data_points", len(df_processed))
-            
-            result = predict_price(df_processed, ticker=request.ticker.upper(), days=request.days)
-            
-            # Log das previs√µes
-            mlflow.log_metric("last_known_price", result['last_known_price'])
-            for i, pred in enumerate(result['predictions'], 1):
-                mlflow.log_metric(f"prediction_day_{i}", pred)
-            mlflow.log_param("status", "success")
-            
-            logger.info(f"Previs√£o conclu√≠da para {request.ticker}: {request.days} dias")
-            
-            return PredictionResponse(
-                ticker=request.ticker.upper(),
-                predictions=[round(p, 2) for p in result['predictions']],
-                days=result['days'],
-                last_known_price=round(result['last_known_price'], 2)
-            )
+        df_processed = get_or_scrappe_ticker(
+            ticker=request.ticker.upper(),
+            data_path="scapper/data/processed/prices",
+            start_date=request.start_date,
+            end_date=request.end_date
+        )
+        
+        if df_processed is None or df_processed.empty:
+            raise HTTPException(
+                status_code=404,
+                detail=f"N√£o foi poss√≠vel obter dados para o ticker {request.ticker}"
+            )
+        
+        result = predict_price(df_processed, ticker=request.ticker.upper(), days=request.days)
+        
+        logger.info(f"Previs√£o conclu√≠da para {request.ticker}: {request.days} dias")
+        
+        return PredictionResponse(
+            ticker=request.ticker.upper(),
+            predictions=[round(p, 2) for p in result['predictions']],
+            days=result['days'],
+            last_known_price=round(result['last_known_price'], 2)
+        )
         
     except ValueError as e:
         logger.error(f"Erro de valida√ß√£o: {e}")
@@ -259,18 +233,11 @@
         
         logger.info(f"Recebida requisi√ß√£o de predi√ß√£o customizada: {len(request.historical_data)} pontos, {request.days} dias")
         
-        # Iniciar tracking MLFlow
-        with mlflow.start_run(run_name=f"predict_custom_{request.ticker_name}_{request.days}days"):
-            mlflow.log_param("ticker_name", request.ticker_name)
-            mlflow.log_param("days", request.days)
-            mlflow.log_param("endpoint", "POST /predict-custom")
-            mlflow.log_metric("historical_data_points", len(request.historical_data))
-        
-            if len(request.historical_data) < 30:
-                raise HTTPException(
-                    status_code=400,
-                    detail=f"M√≠nimo de 30 pontos hist√≥ricos necess√°rios. Fornecidos: {len(request.historical_data)}"
-                )
+        if len(request.historical_data) < 30:
+            raise HTTPException(
+                status_code=400,
+                detail=f"M√≠nimo de 30 pontos hist√≥ricos necess√°rios. Fornecidos: {len(request.historical_data)}"
+            )
         
         data_dict = {
             'date': [pd.to_datetime(point.date) for point in request.historical_data],
@@ -354,14 +321,6 @@
         
         rmse = np.sqrt(mean_squared_error(reals, preds))
         
-        # Log m√©tricas de treino no MLFlow
-        mlflow.log_metric("rmse", rmse)
-        mlflow.log_metric("train_samples", len(X_train))
-        mlflow.log_metric("test_samples", len(X_test))
-        mlflow.log_param("seq_length", SEQ_LENGTH)
-        mlflow.log_param("epochs", EPOCHS)
-        mlflow.log_param("learning_rate", LEARNING_RATE)
-        
         logger.info(f"RMSE do modelo tempor√°rio: {rmse:.2f}")
         
         predictions = []
@@ -385,12 +344,6 @@
         
         last_known_price = float(df['close'].iloc[-1])
         
-        # Log das previs√µes no MLFlow
-        mlflow.log_metric("last_known_price", last_known_price)
-        for i, pred in enumerate(predictions, 1):
-            mlflow.log_metric(f"prediction_day_{i}", pred)
-        mlflow.log_param("status", "success")
-        
         logger.info(f"Previs√£o customizada conclu√≠da: {request.days} dias")
         
         return CustomPredictionResponse(
Index: .env.example
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># =========================\n# Configura√ß√£o de Armazenamento\n# =========================\n\n# Tipo de armazenamento: 'local' ou 's3'\nSTORAGE_TYPE=local\n\n# Configura√ß√µes S3 (apenas se STORAGE_TYPE=s3)\nS3_BUCKET=teste-s3-dados-tickers\nAWS_ACCESS_KEY_ID=your_access_key\nAWS_SECRET_ACCESS_KEY=your_secret_key\nAWS_DEFAULT_REGION=us-east-1\n\n# =========================\n# Configura√ß√£o MLFlow\n# ‚ö†\uFE0F MLFlow est√° SEMPRE HABILITADO\n# =========================\n\n# URI do servidor de tracking do MLFlow\n# Pode ser: \n# - file:./mlruns (local)\n# - http://mlflow-server:5000 (servidor remoto)\nMLFLOW_TRACKING_URI=file:./mlruns\n\n# Nome do experimento MLFlow\nMLFLOW_EXPERIMENT_NAME=stock-price-prediction\n\n# =========================\n# Configura√ß√£o da API\n# =========================\n\n# Porta da API (opcional)\nAPI_PORT=8000\n\n# Host da API (opcional)\nAPI_HOST=0.0.0.0\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.env.example b/.env.example
--- a/.env.example	(revision 17f2ab735aa08aa0cd60b304b1c1940161aa971d)
+++ b/.env.example	(date 1767837023130)
@@ -13,7 +13,6 @@
 
 # =========================
 # Configura√ß√£o MLFlow
-# ‚ö†Ô∏è MLFlow est√° SEMPRE HABILITADO
 # =========================
 
 # URI do servidor de tracking do MLFlow
Index: .gitignore
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>.venv/\n__pycache__\nmlruns/\nmlartifacts/\n.env
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.gitignore b/.gitignore
--- a/.gitignore	(revision 17f2ab735aa08aa0cd60b304b1c1940161aa971d)
+++ b/.gitignore	(date 1767837023130)
@@ -1,5 +1,2 @@
 .venv/
-__pycache__
-mlruns/
-mlartifacts/
-.env
\ No newline at end of file
+__pycache__
\ No newline at end of file
